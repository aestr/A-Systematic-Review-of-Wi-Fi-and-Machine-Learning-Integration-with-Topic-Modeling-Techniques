{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aestr/A-Systematic-Review-of-Wi-Fi-and-Machine-Learning-Integration-with-Topic-Modeling-Techniques/blob/main/Combined_scripts_RA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting Google Drive or GIT clone"
      ],
      "metadata": {
        "id": "Zurr8dpZmM59"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M6Oonw-eH3o7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b11956-9ea1-4dc4-c4eb-48bc744e19a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyaki4-9fumx",
        "outputId": "3952ade3-fe59-4283-8d24-2e001b5aa30b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aestr/A-Systematic-Review-of-Wi-Fi-and-Machine-Learning-Integration-with-Topic-Modeling-Techniques"
      ],
      "metadata": {
        "id": "ExWP-rVyD242",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d109f5a-3be5-4e61-bb54-75878a38f555"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'A-Systematic-Review-of-Wi-Fi-and-Machine-Learning-Integration-with-Topic-Modeling-Techniques'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 65 (delta 22), reused 13 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (65/65), 18.81 MiB | 6.57 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd A-Systematic-Review-of-Wi-Fi-and-Machine-Learning-Integration-with-Topic-Modeling-Techniques/"
      ],
      "metadata": {
        "id": "ht8gFn21EXoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3291b95b-36e7-414c-ef10-e2ce6f98fbc8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/A-Systematic-Review-of-Wi-Fi-and-Machine-Learning-Integration-with-Topic-Modeling-Techniques\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Required libraries/packages"
      ],
      "metadata": {
        "id": "9JnHrqqLm1qP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "General"
      ],
      "metadata": {
        "id": "wXxjewSSnAZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pyBibX"
      ],
      "metadata": {
        "id": "ellrtmGXXWft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8LN3HbC_BoJ"
      },
      "outputs": [],
      "source": [
        "############################################################################\n",
        "\n",
        "# Created by: Prof. Valdecy Pereira, D.Sc.\n",
        "# UFF - Universidade Federal Fluminense (Brazil)\n",
        "# email:  valdecy.pereira@gmail.com\n",
        "# pyBibX - A Bibliometric and Scientometric Library\n",
        "# Example - Scopus\n",
        "\n",
        "# Citation:\n",
        "# PEREIRA, V. (2022). Project: pyBibX, File: pbibx.py, GitHub repository: <https://github.com/Valdecy/pyBibX>\n",
        "\n",
        "############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ip99bnK3_BoK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3248187c-0ba6-45fc-c828-6bb1213c0b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/92.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m92.2/92.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.8/92.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.8/90.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyBibX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Xl-I-RZf_BoK"
      },
      "outputs": [],
      "source": [
        "# Required Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "from pyBibX.base import pbx_probe\n",
        "from google.colab import data_table\n",
        "from prettytable import PrettyTable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Litstudy"
      ],
      "metadata": {
        "id": "KcWcogrdnrBX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8mrKG0TNXuH"
      },
      "source": [
        "Litstudy approach to retrieve bibliographic data from Scopus or another source (i.e. in RIS format) and execute processes shown in https://nlesc.github.io/litstudy/example.html for analysis and interpretation. Reference for litstudy: https://www.sciencedirect.com/science/article/pii/S235271102200125X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94OnP0mNNGo2"
      },
      "outputs": [],
      "source": [
        "!pip install -q litstudy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUT97HgxNhbP"
      },
      "outputs": [],
      "source": [
        "# Import other libraries\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4Mk-rhBNo_e"
      },
      "outputs": [],
      "source": [
        "# Options for plots\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "sbs.set('paper')\n",
        "\n",
        "# Import litstudy\n",
        "path = os.path.abspath(os.path.join('..'))\n",
        "if path not in sys.path:\n",
        "    sys.path.append(path)\n",
        "\n",
        "import litstudy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keyword extraction from curriculum and curation"
      ],
      "metadata": {
        "id": "j1zo1peDrnx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your text\n",
        "text = \"\"\"\n",
        "13872 Grapevine Sciences\n",
        "\n",
        "214 (12) Grapevine plant materials and their growth and metabolism (2L, 3P)\n",
        "Grapevine resources for wine and table grape production (rootstock and scion cultivars and varieties); ampelography; seasonal cycles; vine growth and metabolism.\n",
        "Method of assessment: Flexible assessment Home department: Viticulture and Oenology\n",
        "\n",
        "244 (16) Resource allocation and physiology of grapevines (3L, 3P)\n",
        "Resource allocation and physiology of grapevines, the latter including vegetative, reproductive, ripening and stress physiology.\n",
        "Method of assessment: Flexible assessment Prequisite module: Grapevine Sciences 214 Home department: Viticulture and Oenology\n",
        "\n",
        "312 (8) Table and raisin grape production (2L, 3P)\n",
        "The global industries. Climate and other requirements for table and raisin grape production. Cultivars, rootstocks, nursery vine quality. Vegetative and reproductive development. Trellis systems and vine development. Production practices linked to the seasonal cycle of the grapevine (pruning, dormancy management, canopy management, crop control, bunch preparation). External and internal fruit quality. Maturity indexing, harvest and post-harvest practices. Compiling production, harvest and post-harvest plans for two table grape cultivars (one labour intensive and one not labour intensive) OR for two raisin grape cultivars. Case study of a commercial unit’s implementation of a production plan, as well as the harvest and post-harvest processes of these two cultivars.\n",
        "Method of assessment: Flexible assessment Home department: Viticulture and Oenology\n",
        "\n",
        "314 (12) Grapevine improvement/propagation, establishment and cultivation practices (2L, 3P)\n",
        "The improvement and propagation of grapevine material, grapevine development and the maintenance of grapevines through pruning. Detailed knowledge of nursery practices, grapevine planting and training, and pruning systems and their application in different scenarios will be shared.\n",
        "Method of assessment: Flexible assessment Prerequisite modules: Grapevine Sciences 214, 244 Home department: Viticulture and Oenology\n",
        "\n",
        "344 (12) Trellising systems and canopy management: pests, disease and abnormalities (2L, 3P)\n",
        "The basis for choosing the appropriate training/trellising system and the appropriate canopy management programme. The identification of pests, diseases and abnormalities (including nutrient deficiency/toxicity) is covered, along with appropriate interventions.\n",
        "Method of assessment: Flexible assessment Corequisite module: Grapevine Sciences 314\n",
        "\n",
        "Home department: Viticulture and Oenology\n",
        "\n",
        "444 (16) Advanced viticulture (3L, 3T, 3P)\n",
        "This module provides a theoretical and practical basis for identifying and managing variability within vineyards, with a focus on maximising yield and quality while minimising environmental impacts by optimizing the use of natural resources (soil and water) and chemical applications (fertilizers, and pesticides and herbicides). The implementation of this concept is accomplished by the analysis of local variation in factors that influence grapevine yield and quality (soil, topography, microclimate, vine health, vegetative growth, etc.) using remote sensing techniques (proximal sensors, aerial platforms and satellites) in combination with geographic information system (GIS) and basic geostatistics principles for generating spatial variability maps of the vineyards.\n",
        "Method of assessment: Flexible assessment Prequisite module: Grapevine Sciences 344 Home department: Viticulture and Oenology\n",
        "\n",
        "452 (8) Grape farming systems and business models (2L, 3P)\n",
        "Table and raisin grape production systems to produce table grapes/raisins for desired quality and market requirement outcomes. Market access 2-day accredited short course (including GLOBALGAP or similar quality traceability system), compiling production, harvest and post-harvest plans for a commercial unit. Case study of a commercial unit’s implementation of a production plan, as well as the harvest and post-harvest processes.\n",
        "Method of assessment: Flexible assessment Prequisite modules: Grapevine Sciences 314, 344 Corequisite module: Grapevine Sciences 444 Home department: Viticulture and Oenology\n",
        "\n",
        "13710 Grapevine and Wine Sciences\n",
        "\n",
        "142 (8) Introduction to grapevine and wine sciences (1.5L, 1.5P)\n",
        "Basic grape morphology and production directions. Wine grape cultivars. An introduction to the composition of grapes, must and wine, as well as micro-organisms in winemaking. The fundamentals of alcoholic fermentation, winery equipment and production methods. An introduction to wine styles and wine evaluation.\n",
        "Method of assessment: Flexible assessment Home department: Viticulture and Oenology\n",
        "\n",
        "212 (8) Introduction to grapevine and wine microbiology (1.5L, 1.5P)\n",
        "History of wine microbiology, description of micro-organisms associated with the grapevine and wine environments and practical ways to isolate, identify and manage their growth, basic biochemical pathways pertaining to wine fermentation.\n",
        "Method of assessment: Flexible assessment Home department: Viticulture and Oenology\n",
        "\n",
        "278 (8) Practical Project (1): Integrated grapevine and wine sciences (2L, 2P)\n",
        "Application of viticultural and oenological knowledge contained in first- and second-year modules in which critical academic skills are demonstrated. Presentation of a photographic/electronic portfolio, a literature review, a vineyard plan and a wine tasting.\n",
        "Method of assessment: Flexible assessment Home department: Viticulture and Oenology\n",
        "378 (16) Practical Project (2): Integrated grapevine and wine sciences (2L, 2P)\n",
        "Application of viticultural and oenological knowledge contained in second- and third-year modules in which critical academic skills are demonstrated. Presentation of scientific reports, portfolios, process flow charts, compliant wine labels, a basic marketing plan and a wine tasting.\n",
        "Method of assessment: Flexible assessment Prerequisite module: Grapevine and Wine Sciences 278 Home department: Viticulture and Oenology\n",
        "\n",
        "444 (8) International terroir and wines (2L, 3P)\n",
        "This module introduces the student to the terroir concept in grapevine and wine science, and explores viticultural management practices and wine style decision-making under ‘normal’ and rapidly -changing climatic conditions. It introduces the main\n",
        "\n",
        "characteristics (typicality) of international and local wines associated with specific terroirs.\n",
        "Method of assessment: Flexible assessment Prerequisite module: Grapevine Sciences 344 Home department: Viticulture and Oenology\n",
        "\n",
        "454 (8) The Future of Wine (2L, 3P)\n",
        "The module will provide an overview of the drivers of change in the wine industry, the relevance of innovation in the context of a changing world, the process of innovation, and evaluation of technologies that have the potential to disrupt the current attitudes and practices in the wine industry.\n",
        "Method of assessment: Flexible assessment Home department: Viticulture and Oenology\n",
        "478 (60) Industry Internship (3T, 3P)\n",
        "This module utilises a work-integrated learning strategy to enhance practical viticultural and winemaking experience in the industry under the guidance of academic and industry mentors. Experience in all aspects of cellar and vineyard management. Identification and design of a scientific research project or system in the workplace. Working in teams and individually to manage vines, monitor ripening, produce wine, conduct experiment, write a project report and present results and write a reflection on experience.\n",
        "Method of assessment: Flexible assessment\n",
        "Prerequisite pass modules:\n",
        "•\tGrapevine Sciences 214, 244, 314\n",
        "•\tWine Sciences 214, 244, 314\n",
        "•\tGrapevine and Wine Sciences 278 Prerequisite modules:\n",
        "•\tGrapevine Sciences 344\n",
        "•\tWine Sciences 344\n",
        "•\tGrapevine and Wine Sciences 378 Home department: Viticulture and Oenology\n",
        "\"\"\"\n",
        "\n",
        "# Define custom stopwords\n",
        "custom_stopwords = [\"module\", \"•\", \"'\", \"5l\", \"`\", \"5p\", \"2p\", \"assessment\", \"sciences\", \"method\", \"prerequisite\",\"prequisite\", \"corequisite\", \"department\", \"flexible\", \"home\", \"3p\", \"modules\", \"2l\"]\n",
        "\n",
        "# Manually add custom stopwords that may not be considered\n",
        "custom_stopwords.extend([\"custom_stopword1\", \"custom_stopword2\"])\n",
        "\n",
        "# Combine custom stopwords with NLTK's stopwords\n",
        "combined_stopwords = set(custom_stopwords + stopwords.words('english'))\n",
        "\n",
        "# Function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    text_without_codes = re.sub(r'\\b\\d+\\b', '', text)\n",
        "    words = word_tokenize(text_without_codes)\n",
        "    words_filtered = [word for word in words if word.lower() not in combined_stopwords]\n",
        "    return ' '.join(words_filtered)\n",
        "\n",
        "# Preprocess the text\n",
        "preprocessed_text = preprocess_text(text)\n",
        "\n",
        "# Create a Rake object with custom settings\n",
        "r = Rake(\n",
        "    min_length=1,\n",
        "    max_length=3,\n",
        "    ranking_metric=1,  # Use Degree to Frequency ratio\n",
        "    language=\"english\",\n",
        ")\n",
        "\n",
        "# Extract keywords\n",
        "r.extract_keywords_from_text(preprocessed_text)\n",
        "\n",
        "# Get word frequency distribution\n",
        "word_freq = r.get_word_frequency_distribution()\n",
        "\n",
        "# Filter keywords with a minimum length of 4 letters\n",
        "filtered_keywords = {k: v for k, v in word_freq.items() if len(k) >= 4}\n",
        "\n",
        "# Sort filtered keywords by frequency in descending order\n",
        "sorted_keywords = sorted(filtered_keywords.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Define the number of columns for display\n",
        "num_columns = 4\n",
        "\n",
        "# Calculate the number of keywords per column\n",
        "num_keywords_per_column = len(sorted_keywords) // num_columns\n",
        "\n",
        "# Create a PrettyTable for each column\n",
        "tables = []\n",
        "\n",
        "for i in range(num_columns):\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"Keyword\", \"Frequency\"]\n",
        "    start_idx = i * num_keywords_per_column\n",
        "    end_idx = (i + 1) * num_keywords_per_column\n",
        "    keywords_chunk = sorted_keywords[start_idx:end_idx]\n",
        "    for keyword, freq in keywords_chunk:\n",
        "        table.add_row([keyword, freq])\n",
        "    tables.append(table)\n",
        "\n",
        "# Generate HTML for each table\n",
        "tables_html = [table.get_html_string() for table in tables]\n",
        "\n",
        "# Combine tables horizontally using HTML and display in Markdown cell\n",
        "combined_html = \"<div style='display: flex;'>\" + \"\".join(tables_html) + \"</div>\"\n",
        "HTML(combined_html)\n"
      ],
      "metadata": {
        "id": "tigQmOuSGvxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(sorted_keywords)"
      ],
      "metadata": {
        "id": "EGPigVFBcKi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to df\n",
        "import pandas as pd\n",
        "df_keywords = pd.DataFrame(sorted_keywords, columns=['Keywords', 'Frequency'])\n",
        "print(df_keywords)"
      ],
      "metadata": {
        "id": "gZc_v8KkgJUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipywidgets\n",
        "!jupyter nbextension enable --py widgetsnbextension"
      ],
      "metadata": {
        "id": "osOuRtdKoHjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "categories = {\n",
        "    \"Planning and Site Selection\": [],\n",
        "    \"Production and Management\": [],\n",
        "    \"Processing and Packaging\": [],\n",
        "    \"Marketing and Sales\": [],\n",
        "}"
      ],
      "metadata": {
        "id": "YWqvgRanoNFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_keyword(category, keyword):\n",
        "    categories[category].append(keyword)\n",
        "\n",
        "def display_categories():\n",
        "    for category, keywords in categories.items():\n",
        "        print(f\"{category}: {', '.join(keywords)}\")"
      ],
      "metadata": {
        "id": "53fzkxLeoTwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "categories = {\n",
        "    \"Planning and Site Selection\": [],\n",
        "    \"Production and Management\": [],\n",
        "    \"Processing and Packaging\": [],\n",
        "    \"Marketing and Sales\": [],\n",
        "}\n",
        "\n",
        "# Create checkboxes for each keyword for each category\n",
        "checkboxes = {category: [widgets.Checkbox(\n",
        "    value=False,\n",
        "    description=keyword[0],\n",
        ") for keyword in sorted_keywords] for category in categories}\n",
        "\n",
        "# Create a text input for custom keywords for each category\n",
        "custom_keyword_inputs = {category: widgets.Text(\n",
        "    placeholder=f\"Enter a custom keyword for {category}\",\n",
        ") for category in categories}\n",
        "\n",
        "# Create a button to add custom keywords for each category\n",
        "add_custom_keyword_buttons = {category: widgets.Button(\n",
        "    description=f\"Add Custom Keyword for {category}\",\n",
        ") for category in categories}\n",
        "\n",
        "# Create a button to finish inputting\n",
        "finish_button = widgets.Button(\n",
        "    description=\"Finish Inputting\",\n",
        ")\n",
        "\n",
        "# Dictionary to keep track of custom keywords for each category\n",
        "custom_keywords = {category: [] for category in categories}\n",
        "\n",
        "def add_custom_keyword_button_clicked(category):\n",
        "    custom_keyword = custom_keyword_inputs[category].value\n",
        "    if custom_keyword:\n",
        "        custom_keywords[category].append(custom_keyword)\n",
        "        custom_keyword_inputs[category].value = \"\"\n",
        "\n",
        "for category in categories:\n",
        "    add_custom_keyword_buttons[category].on_click(lambda _, category=category: add_custom_keyword_button_clicked(category))\n",
        "\n",
        "# Create a button to display the current state of categorization\n",
        "display_button = widgets.Button(\n",
        "    description=\"Display Categories\",\n",
        ")\n",
        "\n",
        "def display_button_clicked(b):\n",
        "    for category, keywords in categories.items():\n",
        "        selected_keywords = [checkbox.description for checkbox in checkboxes[category] if checkbox.value]\n",
        "        print(f\"{category}: {', '.join(keywords + selected_keywords + custom_keywords[category])}\")\n",
        "\n",
        "display_button.on_click(display_button_clicked)\n",
        "\n",
        "# Display the widgets\n",
        "for category in categories:\n",
        "    display(widgets.HTML(f\"<b>{category}</b>\"))\n",
        "    for checkbox in checkboxes[category]:\n",
        "        display(checkbox)\n",
        "    display(custom_keyword_inputs[category], add_custom_keyword_buttons[category])\n",
        "\n",
        "display(finish_button, display_button)\n",
        "\n",
        "# Function to check if input is finished\n",
        "def finish_button_clicked(b):\n",
        "    finish_button.disabled = True  # Disable the finish button\n",
        "    print(\"Final Categorization:\")\n",
        "    display_button_clicked(None)  # Display the final categorization\n",
        "\n",
        "finish_button.on_click(finish_button_clicked)\n"
      ],
      "metadata": {
        "id": "OzwQiivQoZN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "categories = {\n",
        "    \"Planning and Site Selection\": [],\n",
        "    \"Production and Management\": [],\n",
        "    \"Processing and Packaging\": [],\n",
        "    \"Marketing and Sales\": [],\n",
        "}\n",
        "\n",
        "# Create checkboxes for each keyword for each category\n",
        "checkboxes = {category: [widgets.Checkbox(\n",
        "    value=False,\n",
        "    description=keyword[0],\n",
        ") for keyword in sorted_keywords] for category in categories}\n",
        "\n",
        "# Create a text input for custom keywords for each category\n",
        "custom_keyword_inputs = {category: widgets.Text(\n",
        "    placeholder=f\"Enter a custom keyword for {category}\",\n",
        ") for category in categories}\n",
        "\n",
        "# Create a button to add custom keywords for each category\n",
        "add_custom_keyword_buttons = {category: widgets.Button(\n",
        "    description=f\"Add Custom Keyword for {category}\",\n",
        ") for category in categories}\n",
        "\n",
        "# Create a button to finish inputting\n",
        "finish_button = widgets.Button(\n",
        "    description=\"Finish Inputting\",\n",
        ")\n",
        "\n",
        "# Dictionary to keep track of custom keywords for each category\n",
        "custom_keywords = {category: [] for category in categories}\n",
        "\n",
        "def add_custom_keyword_button_clicked(category):\n",
        "    custom_keyword = custom_keyword_inputs[category].value\n",
        "    if custom_keyword:\n",
        "        custom_keywords[category].append(custom_keyword)\n",
        "        custom_keyword_inputs[category].value = \"\"\n",
        "\n",
        "for category in categories:\n",
        "    add_custom_keyword_buttons[category].on_click(lambda _, category=category: add_custom_keyword_button_clicked(category))\n",
        "\n",
        "# Create a button to display the current state of categorization\n",
        "display_button = widgets.Button(\n",
        "    description=\"Display Categories\",\n",
        ")\n",
        "\n",
        "def display_button_clicked(b):\n",
        "    for category, keywords in categories.items():\n",
        "        selected_keywords = [checkbox.description for checkbox in checkboxes[category] if checkbox.value]\n",
        "        print(f\"{category}: {', '.join(keywords + selected_keywords + custom_keywords[category])}\")\n",
        "\n",
        "display_button.on_click(display_button_clicked)\n",
        "\n",
        "# Display the widgets\n",
        "column_layout = widgets.Layout(grid_template_columns=\"repeat(3, 200px)\")  # Adjust the number of columns as needed\n",
        "\n",
        "for category in categories:\n",
        "    category_widgets = []\n",
        "    for checkbox in checkboxes[category]:\n",
        "        category_widgets.append(checkbox)\n",
        "    category_widgets.extend([custom_keyword_inputs[category], add_custom_keyword_buttons[category]])\n",
        "    category_grid = widgets.GridBox(category_widgets, layout=column_layout)\n",
        "    display(widgets.HTML(f\"<b>{category}</b>\"))\n",
        "    display(category_grid)\n",
        "\n",
        "display(finish_button, display_button)\n",
        "\n",
        "# Function to check if input is finished\n",
        "def finish_button_clicked(b):\n",
        "    finish_button.disabled = True  # Disable the finish button\n",
        "    print(\"Final Categorization:\")\n",
        "    display_button_clicked(None)  # Display the final categorization\n",
        "\n",
        "finish_button.on_click(finish_button_clicked)\n"
      ],
      "metadata": {
        "id": "iDjiou5MsjaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ... (Previous code for categorizing keywords)\n",
        "\n",
        "# Create a list to store all keywords and their categories\n",
        "all_keywords = []\n",
        "\n",
        "# Iterate through the categories and keywords\n",
        "for category, keywords in categories.items():\n",
        "    for keyword in keywords:\n",
        "        all_keywords.append((keyword, category))\n",
        "\n",
        "# Iterate through custom keywords\n",
        "for category, custom_keywords_list in custom_keywords.items():\n",
        "    for custom_keyword in custom_keywords_list:\n",
        "        all_keywords.append((custom_keyword, category))\n",
        "\n",
        "# Iterate through selected keywords from the sorted list using radio buttons\n",
        "for category in categories:\n",
        "    selected_keywords = [checkbox.description for checkbox in checkboxes[category] if checkbox.value]\n",
        "    for keyword in selected_keywords:\n",
        "        all_keywords.append((keyword, category))\n",
        "\n",
        "# Convert the list of tuples into a DataFrame\n",
        "keywords_categorized = pd.DataFrame(all_keywords, columns=[\"Keyword\", \"Category\"])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(keywords_categorized)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "keywords_categorized.to_csv(\"keywords_categorized.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "wUWCmebutx06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh803dQMXWip"
      },
      "source": [
        "keywords_categorized.at[2, 'Keyword'] = 'site selection'\n",
        "keywords_categorized # This is after the change of value (Microsoft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame to a CSV file\n",
        "keywords_categorized.to_csv(\"keywords_categorized.csv\", index=False)"
      ],
      "metadata": {
        "id": "yKVPrB2UFcdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "\n",
        "# Display the word cloud using matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kb7-YUdFtyH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading from database generated files"
      ],
      "metadata": {
        "id": "DDaSot-KmbDe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9orhNMeb_BoL"
      },
      "outputs": [],
      "source": [
        "# Load .bib\n",
        "# Arguments: file_bib = 'filename.bib'; db = 'scopus', 'wos', 'pubmed'; del_duplicated = True, False\n",
        "file_name = '/content/drive/MyDrive/MEM RA data/4IR.bib'\n",
        "database  = 'scopus'\n",
        "bibfile   = pbx_probe(file_bib = file_name, db = database, del_duplicated = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc4tolTZiH-d"
      },
      "outputs": [],
      "source": [
        "# Load .bib\n",
        "# Arguments: file_bib = 'filename.bib'; db = 'scopus', 'wos', 'pubmed'; del_duplicated = True, False\n",
        "file_name = '/content/drive/MyDrive/MEM RA data/agri emerging tech viti wine.bib'\n",
        "database  = 'scopus'\n",
        "bibfile   = pbx_probe(file_bib = file_name, db = database, del_duplicated = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvc7G1T1iLqy"
      },
      "outputs": [],
      "source": [
        "# Load .bib\n",
        "# Arguments: file_bib = 'filename.bib'; db = 'scopus', 'wos', 'pubmed'; del_duplicated = True, False\n",
        "file_name = '/content/drive/MyDrive/MEM RA data/4IR viti wine.bib'\n",
        "database  = 'scopus'\n",
        "bibfile   = pbx_probe(file_bib = file_name, db = database, del_duplicated = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnWYtS7iQVLA"
      },
      "outputs": [],
      "source": [
        "# Load .bib\n",
        "# Arguments: file_bib = 'filename.bib'; db = 'scopus', 'wos', 'pubmed'; del_duplicated = True, False\n",
        "file_name = '/content/drive/MyDrive/MEM RA data/processing packaging.bib'\n",
        "database  = 'scopus'\n",
        "bibfile   = pbx_probe(file_bib = file_name, db = database, del_duplicated = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7bqjajFzXC_"
      },
      "outputs": [],
      "source": [
        "docs_SS = litstudy.search_semanticscholar('agritech')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ8wNmoZ75ZK"
      },
      "outputs": [],
      "source": [
        "docs_SS = litstudy.load_ris_file('/content/drive/MyDrive/MEM RA data/scopus viti 4ir.ris')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFpgoF66Puw9"
      },
      "outputs": [],
      "source": [
        "docs_SS = litstudy.load_bibtex('/content/drive/MyDrive/MEM RA data/4IR.bib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfVB9U1gijU0"
      },
      "outputs": [],
      "source": [
        "docs_SS = litstudy.load_bibtex('/content/drive/MyDrive/MEM RA data/4IR viti wine.bib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1xsxYRAWgbW"
      },
      "outputs": [],
      "source": [
        "docs_SS = litstudy.load_bibtex('/content/drive/MyDrive/MEM RA data/processing packaging.bib')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import glob\n",
        "import os\n",
        "# merging the files\n",
        "joined_files = os.path.join(\"/content/drive/MyDrive/MEM RA data\", \"CAB Agritech1*.csv\")\n",
        "# A list of all joined files is returned\n",
        "joined_list = glob.glob(joined_files)"
      ],
      "metadata": {
        "id": "6BDsRdg2ueoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, the files are joined\n",
        "scopus_tot = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)\n",
        "print(scopus_tot)\n",
        "len(scopus_tot)\n",
        "scopus_tot.to_csv('/content/drive/My Drive/MEM RA data/CAB.csv', index=False)"
      ],
      "metadata": {
        "id": "oEVm4Mgmwp-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGHn73LHt9oG"
      },
      "outputs": [],
      "source": [
        "docs_SS = litstudy.load_csv('/content/drive/MyDrive/MEM RA data/CAB.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNcxrVyL3aCo"
      },
      "outputs": [],
      "source": [
        "print(len(docs_SS), 'documents retrieved')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTopic Atzeni"
      ],
      "metadata": {
        "id": "nh3k5l34Qssg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o_T55LlXJSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c26826-22ac-4f8b-8c94-c633ee33413e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4838"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# scopus_1 = pd.read_csv('raw_data/scopus_1.csv')\n",
        "# scopus_2 = pd.read_csv('raw_data/scopus_2.csv')\n",
        "\n",
        "scopus_tot = pd.read_csv('/content/drive/MyDrive/MEM RA data/Agritech1.csv')\n",
        "len(scopus_tot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bXVGGgvqC8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "755ac160-a152-43c1-951f-4a20acd708a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4838"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "scopus_tot.drop_duplicates(inplace=True)\n",
        "len(scopus_tot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GjwKpFn7Hx0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "07df8402-19e2-4955-fcab-7da1a3201bba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Developing product level indicators to advance the nitrogen circular economy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "scopus_tot['Title'].iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6G9f3qe6vR0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "ef314ff0-d1a2-48af-f569-b68095fbdbbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Apodaca L., Minerals Yearbook: Nitrogen, (2018); GREET® Model: The Greenhouse Gases, Regulated Emissions, and Energy Use in Transportation (GREET) Model, (2022); Bian Z., Tian H., Yang Q., Xu R., Pan S., Zhang B., Production and application of manure nitrogen and phosphorus in the United States since 1860, Biosph. Biogeosci., (2020); Bittman S., Dedina M., Howard C.M., Oenema O., Sutton M.A., Options For Ammonia Mitigation: Guidance from the UNECE Task Force On Reactive Nitrogen, (2014); Braakman L., Bhochhibhoya S., de Graaf R., Exploring the relationship between the level of circularity and the life cycle costs of a one-family house, Resour. Conserv. Recycl., 164, (2021); Centner T.J., Regulating the land application of manure from animal production facilities in the USA, Water Policy, 14, 2, pp. 319-335, (2012); Cobo S., Dominguez-Ramos A., Irabien A., Trade-offs between nutrient circularity and environmental impacts in the management of organic waste, Environ. Sci. Technol., 52, 19, pp. 10923-10933, (2018); de Oliveira C.T., Dantas T.E.T., Soares S.R., Nano and micro level circular economy indicators: assisting decision-makers in circularity assessments, Sustain. Prod. Consump., 26, pp. 455-468, (2021); Elia V., Gnoni M.G., Tornese F., Measuring circular economy strategies through index methods: a critical analysis, J. Clean. Prod., 142, pp. 2741-2751, (2017); (2015); A European Strategy for Plastics in a Circular Economy, (2018); Fernandez-Mena H., Gaudou B., Pellerin S., MacDonald G.K., Nesme T., Flows in Agro-food Networks (FAN): an agent-based model to simulate local agricultural material flows, Agric. Syst., 180, (2020); Garza-Reyes J.A., Salome Valls A., Peter Nadeem S., Anosike A., Kumar V., A circularity measurement toolkit for manufacturing SMEs, Int. J. Prod. Res., 57, 23, pp. 7319-7343, (2019); Glogic E., Sonnemann G., Young S.B., Environmental trade-offs of downcycling in circular economy: combining life cycle assessment and material circularity indicator to inform circularity strategies for alkaline batteries, Sustainability, 13, 3, (2021); Goyal S., Chauhan S., Mishra P., Circular economy research: a bibliometric analysis (2000–2019) and future research insights, J. Clean. Prod., 287, (2021); Harder R., Giampietro M., Smukler S., Towards a circular nutrient economy. A novel way to analyze the circularity of nutrient flows in food systems, Resour. Conserv. Recycl., 172, (2021); Animal Feed/Food Consumption and COVID-19 Impact Analysis, (2020); Kakwani, Nikita S., Kalbar P.P., Measuring urban water circularity: development and implementation of a water circularity indicator, Sustain. Prod. Consump., 31, May, pp. 723-735, (2022); Kar S., Singh R., Gurian, Patrick L., Hendricks A., Kohl P., McKelvey S., Spatari S., Life cycle assessment and techno-economic analysis of nitrogen recovery by ammonia air-stripping from wastewater treatment, Sci. Total Environ., 857, (2023); Kirchherr J., Reike D., Hekkert M., Conceptualizing the circular economy: an analysis of 114 definitions, Resour. Conserv. Recycl., 127, pp. 221-232, (2017); Kogler A., Farmer M., Simon J.A., Tilmans S., Wells G.F., Tarpeh W.A., Systematic evaluation of emerging wastewater nutrient removal and recovery technologies to inform practice and advance resource efficiency, ACS ES&T Eng., 1, 4, pp. 662-684, (2021); Kyriakou V., Garagounis I., Vourros A., Vasileiou E., Stoukides M., An electrochemical Haber-Bosch process, Joule, 4, 1, pp. 142-158, (2020); Li B., Brett M.T., The impact of alum based advanced nutrient removal processes on phosphorus bioavailability, Water Res., 46, 3, pp. 837-844, (2012); Liu J., Ma K., Ciais P., Polasky S., Reducing human nitrogen use for food production, Sci. Rep., 6, 1, (2016); Lonca G., Lesage P., Majeau-Bettez G., Bernard S., Margni M., Assessing scaling effects of circular economy strategies: a case study on plastic bottle closed-loop recycling in the USA PET market, Resour. Conserv. Recycl., 162, (2020); Lonca G., Muggeo R., Imbeault-Tetreault H., Bernard S., Margni M., Does material circularity rhyme with environmental efficiency? Case studies on used tires, J. Clean. Prod., 183, pp. 424-435, (2018); Matassa S., Boon N., Pikaar I., Verstraete W., Microbial protein: future sustainable food supply route with low environmental footprint, Microb. Biotechnol., 9, 5, pp. 568-575, (2016); Matassa S., Batstone D.J., Hulsen T., Schnoor J., Verstraete W., Can direct conversion of used nitrogen to new feed and protein help feed the world?, Environ. Sci. Technol., 49, 9, pp. 5247-5254, (2015); Miksch K., Sikora J., Wastewater Biotechnology (In Polish), (2012); Filho O., Daguerre-Martini S., Vanotti M.B., Saez-Tovar J., Rosal A., Perez-Murcia M.D., Bustamante M.A., Moral R., Recovery of ammonia in raw and co-digested swine manure using gas-permeable membrane technology, Front. Sustain. Food Syst., 2, (2018); Pandey B., Chen L., Technologies to recover nitrogen from livestock manure—a review, Sci. Total Environ., 784, (2021); Papangelou A., Achten W.M.J., Mathijs E., Phosphorus and energy flows through the food system of Brussels Capital Region, Resour. Conserv. Recycl., 156, (2020); Preisner M., Smol M., Horttanainen M., Deviatkin I., Havukainen J., Klavins M., Ozola-Davidane R., Kruopiene J., Szatkowska B., Appels L., Houtmeyers S., Roosalu K., Indicators for resource recovery monitoring within the circular economy model implementation in the wastewater sector, J. Environ. Manage., 304, (2022); Qin Z., Canter C.E., Dunn J.B., Mueller S., Kwon H., Han J., Wander M.M., Wang M., “Incorporating agricultural management practices into the assessment of soil carbon change and life-cycle greenhouse gas emissions of corn stover ethanol production, (2015); Qin Z., Canter C.E., Dunn J.B., Mueller S., Kwon H., Han J., Wander M.M., Wang M., Land management change greatly impacts biofuels’ greenhouse gas emissions, GCB Bioenergy, 10, pp. 370-381, (2018); Rocchi L., Paolotti L., Cortina C., Fagioli F.F., Boggia A., Measuring circularity: an application of modified material circularity indicator to agricultural systems, Agric. Food Econ., 9, 1, (2021); Rossi E., Bertassini A.C., Ferreira C.D.S., Neves do Amaral W.A., Ometto A.R., Circular economy indicators for organizations considering sustainability and business models: plastic, textile, and electro-electronic cases, J. Clean. Prod., 247, (2020); Shoji S., Delgado J., Mosier A., Miura Y., Use of controlled release fertilizers and nitrification inhibitors to increase nitrogen use efficiency and to conserve air and water quality, Commun. Soil Sci. Plant Anal., 32, 7-8, pp. 1051-1070, (2001); Scherson Y.D., Woo S.G., Criddle C.S., Production of nitrous oxide from anaerobic digester centrate and its use as a co-oxidant of biogas to enhance energy recovery, Environ. Sci. Technol., 48, 10, pp. 5612-5619, (2014); Scherson Y.D., Wells G.F., Woo S.G., Lee J., Park J., Cantwell B.J., Criddle C.S., Nitrogen removal with energy recovery through N<sub>2</sub>O decomposition, Energy Environ. Sci., 6, 1, pp. 241-248, (2013); Shaddel S., Ucar S., Andreassen J.P., Osterhus S.W., Enhancing efficiency and economics of phosphorus recovery process by customizing the product based on sidestream characteristics – an alternative phosphorus recovery strategy, Water Sci. Technol., 79, 9, pp. 1777-1789, (2019); Tadesse S.T., Oenema O., van Beek C., Ocho F.L., Nitrogen allocation and recycling in peri-urban mixed crop–livestock farms in Ethiopia, Nutrient Cycl. Agroecosyst., 115, 2, pp. 281-294, (2019); Uriarte A.K., Rodkin M.A., Gross M.J., Kharitonov A.S., Panov G.I., Direct hydroxylation of benzene to phenol by nitrous oxide, Studies in Surface Science and Catalysis, 110, pp. 857-864, (1997); U.S Plastics Report 2020 Baseline Report, (2020); Van Loon M.P., Wytse J.V., Renske Hijbeek, Van Ittersum M.K., Ten Berge H.F.M., Circularity indicators and their relation with nutrient use efficiency in agriculture and food systems, Agric. Syst., 207, (2023); Velasco-Munoz J.F., Mendoza J.M.F., Aznar-Sanchez J.A., Gallego-Schmid A., Circular economy implementation in the agricultural sector: definition, strategies and indicators, Resour. Conserv. Recycl., 170, (2021); Verberne J.J., Building Circularity Indicators: an Approach for Measuring Circularity of a Building, (2016); Wang Z., Hellweg S., First steps toward sustainable circular uses of chemicals: advancing the assessment and management paradigm, ACS Sustain. Chem. Eng., 9, 20, pp. 6939-6951, (2021); Xu X., Sharma P., Shu S., Lin T.S., Ciais P., Tubiello F.N., Smith P., Campbell N., Jain A.K., Global greenhouse gas emissions from animal-based foods are twice those of plant-based foods, Nature Food, 2, 9, pp. 724-732, (2021); Xue M., Lin B., Tsunemi K., Minami K., Nanba T., Kawamoto T., Life cycle assessment of nitrogen circular economy-based NO<sub>x</sub> treatment technology, Sustainability, 13, 14, (2021)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "scopus_tot['References'].iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2SjHal0qLlf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6928bb6-fc74-40e3-e6c1-011a4cefd4e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Authors', 'Author full names', 'Author(s) ID', 'Title', 'Year',\n",
              "       'Source title', 'Volume', 'Issue', 'Art. No.', 'Page start', 'Page end',\n",
              "       'Page count', 'Cited by', 'DOI', 'Link', 'Affiliations',\n",
              "       'Authors with affiliations', 'Abstract', 'Author Keywords',\n",
              "       'Index Keywords', 'Molecular Sequence Numbers', 'Chemicals/CAS',\n",
              "       'Tradenames', 'Manufacturers', 'Funding Details', 'Funding Texts',\n",
              "       'References', 'Correspondence Address', 'Editors', 'Publisher',\n",
              "       'Sponsors', 'Conference name', 'Conference date', 'Conference location',\n",
              "       'Conference code', 'ISSN', 'ISBN', 'CODEN', 'PubMed ID',\n",
              "       'Language of Original Document', 'Abbreviated Source Title',\n",
              "       'Document Type', 'Publication Stage', 'Open Access', 'Source', 'EID'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "scopus_tot.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrxn5Qc_rPLQ"
      },
      "outputs": [],
      "source": [
        "scopus_rel_columns = ['Title', 'Year', 'Cited by', 'Abstract', 'Author Keywords', 'Authors', 'Document Type']\n",
        "scopus_tot = scopus_tot[scopus_rel_columns]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3cfX_wJXz_k"
      },
      "source": [
        "# Wos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6IZ6J2ILX1g0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf5f391-f5e3-4977-d964-70e55ad16911"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3547"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "wos_1 = pd.read_excel('raw_data/WOS Agritech1.xls')\n",
        "wos_2 = pd.read_excel('raw_data/WOS Agritech2.xls')\n",
        "wos_3 = pd.read_excel('raw_data/WOS Agritech3.xls')\n",
        "wos_4 = pd.read_excel('raw_data/WOS Agritech4.xls')\n",
        "wos_tot = pd.concat((wos_1, wos_2, wos_3, wos_4))\n",
        "len(wos_tot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "f8VnVdcRqzpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f684fe5-bf71-43cd-f975-f2b90ac664e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3547"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "wos_tot.drop_duplicates(inplace=True)\n",
        "len(wos_tot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSVsP3bkq3XA"
      },
      "outputs": [],
      "source": [
        "wos_tot.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0iKCr8Rr2cM"
      },
      "outputs": [],
      "source": [
        "wos_rel_columns = ['Article Title', 'Author Keywords', 'Abstract', 'Times Cited, All Databases', 'Publication Year', 'Authors', 'Document Type']\n",
        "wos_tot = wos_tot[wos_rel_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjbxuhbQgQgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6988643f-0f75-4724-c41a-2533658ea2a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Valle-Cruz, D; Gil-Garcia, JR', 'Lavallais C.M.; Dunn J.B.')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "wos_tot.iloc[0]['Authors'], scopus_tot.iloc[0]['Authors']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9XOhZxigF2t"
      },
      "outputs": [],
      "source": [
        "# convert authors name as in Scopus\n",
        "def conv_auth_names(x):\n",
        "    if type(x) != str:\n",
        "        return x\n",
        "    authors = x.split(';')\n",
        "    res = ''\n",
        "    for i, el in enumerate(authors):\n",
        "        # horrible, but we must avoid names without initials\n",
        "        if ',' not in el:\n",
        "            el += ','\n",
        "        surn, inits = el.split(',')\n",
        "        inits = inits[1:]   # remove initial space\n",
        "        new_inits = ''.join([c + '.' for c in inits])\n",
        "        res += f'{surn} {new_inits}'\n",
        "        if i != len(authors) - 1:\n",
        "            res += ','  # additional space not needed\n",
        "    return res\n",
        "\n",
        "wos_tot['Authors'] = wos_tot['Authors'].map(lambda x: conv_auth_names(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utK7w-DJnsVx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "0507df1a-2fc2-4c33-feb8-a56b6aec3ee6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Title  Year  Cited by  \\\n",
              "0  Emerging Technologies in Local Government: A S...  2022         0   \n",
              "1  Old Oranges in New Boxes? Strategic Partnershi...  2014        17   \n",
              "2  Industry 4.0 digital technologies enhancing su...  2023         1   \n",
              "3  Risk identification in food safety: Strategy a...  2017        17   \n",
              "4       Emerging and Re-Emerging Foodborne Pathogens  2018        26   \n",
              "\n",
              "                                            Abstract  \\\n",
              "0  Emerging technologies have the potential to tr...   \n",
              "1  Partnerships have recently gained increasing p...   \n",
              "2                                            Editor:   \n",
              "3  The European Food Safety Authority (EFSA) esta...   \n",
              "4  Emergence and re-emergence of foodborne pathog...   \n",
              "\n",
              "                                     Author Keywords  \\\n",
              "0  Emerging Technologies; Public Administration a...   \n",
              "1                                                NaN   \n",
              "2  Industry 4; 0; Emerging economies; Barriers; A...   \n",
              "3  Emerging risk; Preliminary risk assessment; Ea...   \n",
              "4  foodborne pathogen; emerging; re-emerging; zoo...   \n",
              "\n",
              "                                             Authors Document Type  \n",
              "0                     Valle-Cruz D., Gil-Garcia J.R.        Review  \n",
              "1                               Bitzer V., Bijman J.       Article  \n",
              "2  Costa F., Frecassetti S., Rossini M., Portioli...       Article  \n",
              "3  Costa M.C., Goumperis T., Andersson W., Badiol...       Article  \n",
              "4                         Smith J.L., Fratamico P.M.        Review  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ca1a5088-c00c-4958-ae93-c0ebeb1dbacd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Year</th>\n",
              "      <th>Cited by</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Author Keywords</th>\n",
              "      <th>Authors</th>\n",
              "      <th>Document Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Emerging Technologies in Local Government: A S...</td>\n",
              "      <td>2022</td>\n",
              "      <td>0</td>\n",
              "      <td>Emerging technologies have the potential to tr...</td>\n",
              "      <td>Emerging Technologies; Public Administration a...</td>\n",
              "      <td>Valle-Cruz D., Gil-Garcia J.R.</td>\n",
              "      <td>Review</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Old Oranges in New Boxes? Strategic Partnershi...</td>\n",
              "      <td>2014</td>\n",
              "      <td>17</td>\n",
              "      <td>Partnerships have recently gained increasing p...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bitzer V., Bijman J.</td>\n",
              "      <td>Article</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Industry 4.0 digital technologies enhancing su...</td>\n",
              "      <td>2023</td>\n",
              "      <td>1</td>\n",
              "      <td>Editor:</td>\n",
              "      <td>Industry 4; 0; Emerging economies; Barriers; A...</td>\n",
              "      <td>Costa F., Frecassetti S., Rossini M., Portioli...</td>\n",
              "      <td>Article</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Risk identification in food safety: Strategy a...</td>\n",
              "      <td>2017</td>\n",
              "      <td>17</td>\n",
              "      <td>The European Food Safety Authority (EFSA) esta...</td>\n",
              "      <td>Emerging risk; Preliminary risk assessment; Ea...</td>\n",
              "      <td>Costa M.C., Goumperis T., Andersson W., Badiol...</td>\n",
              "      <td>Article</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Emerging and Re-Emerging Foodborne Pathogens</td>\n",
              "      <td>2018</td>\n",
              "      <td>26</td>\n",
              "      <td>Emergence and re-emergence of foodborne pathog...</td>\n",
              "      <td>foodborne pathogen; emerging; re-emerging; zoo...</td>\n",
              "      <td>Smith J.L., Fratamico P.M.</td>\n",
              "      <td>Review</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca1a5088-c00c-4958-ae93-c0ebeb1dbacd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ca1a5088-c00c-4958-ae93-c0ebeb1dbacd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ca1a5088-c00c-4958-ae93-c0ebeb1dbacd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e17adffa-1b72-4685-a228-1ad9577f90a7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e17adffa-1b72-4685-a228-1ad9577f90a7')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e17adffa-1b72-4685-a228-1ad9577f90a7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# scopus rel columns ['Title', 'Year', 'Cited by', 'Abstract', 'Author Keywords', 'Authors', 'Document Type']\n",
        "wos_tot.rename(columns={'Article Title':'Title', 'Times Cited, All Databases':'Cited by', 'Publication Year':'Year'}, inplace=True)\n",
        "\n",
        "wos_tot = wos_tot[scopus_rel_columns]\n",
        "wos_tot.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38X-M5DfNxJF"
      },
      "source": [
        "Test using 1000 records from Semantic Scholar on \"Agricultur* AND Technolog*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsZtPwMa2Glp"
      },
      "outputs": [],
      "source": [
        "!pip install -q semanticscholar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3EILbkl6Eyi"
      },
      "outputs": [],
      "source": [
        "from semanticscholar import SemanticScholar\n",
        "sch = SemanticScholar()\n",
        "docs_SS = sch.search_paper('agriculture AND technology AND emerging', year=2022)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_Uekds84dPB"
      },
      "source": [
        "Now with the API received (see email)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPEu5lU34BDV"
      },
      "outputs": [],
      "source": [
        "from semanticscholar import SemanticScholar\n",
        "s2_api_key = 'y3DWcjw5Ot1vRH99ZRIxd7tWVoc8T5WaaSWMuq9B' # Rate limit 100 requests/second\n",
        "sch = SemanticScholar(api_key=s2_api_key)\n",
        "docs_SS = sch.search_paper('agriculture AND technology AND emerging', fields=['title','year'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKdVW_RE3SuC"
      },
      "outputs": [],
      "source": [
        "print(len(docs_SS), 'documents retrieved')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDd1qqHK6lQH"
      },
      "outputs": [],
      "source": [
        "for item in docs_SS:\n",
        "     print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_1yJ3w3N5le"
      },
      "outputs": [],
      "source": [
        "query = \"agriculture AND technology AND emerging\"\n",
        "limit = 100  # Maximum number of results to return\n",
        "batch_size = 5  # Number of results to retrieve per request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9fPd1qZcRhi"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "session = requests.Session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9lrynMy6jHr"
      },
      "outputs": [],
      "source": [
        "docs_SS = litstudy.search_semanticscholar(\"agritech AND emerging AND viticulture\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGgPiMFY1Ksk"
      },
      "outputs": [],
      "source": [
        "docs_SS = litstudy.search_semanticscholar(query, limit=limit, batch_size=batch_size, session=session)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9U6meMRdnT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-2FcixfrCnA"
      },
      "source": [
        "# Merge DFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABm5Na1PuzH6"
      },
      "outputs": [],
      "source": [
        "scopus_tot.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSNpvcGHu6Ix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f76d54ac-58a5-4973-8ad6-1cb5bfb55dbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8350"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "#df = pd.concat((scopus_tot, wos_tot, ieee))\n",
        "df = pd.concat((scopus_tot, wos_tot))\n",
        "\n",
        "df['Title'] = df['Title'].map(lambda x: x.lower())\n",
        "\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtHxz2bSviAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a168d62-7ef4-44b9-b8bb-0d50080a9afe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6519"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "df.drop_duplicates(subset='Title', inplace=True)\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory analysis"
      ],
      "metadata": {
        "id": "TUE6IZoGmpIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyBibx"
      ],
      "metadata": {
        "id": "4hL-Vdq6pRv-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEykvk6U_BoL"
      },
      "outputs": [],
      "source": [
        "# Generate EDA (Exploratory Data Analysis) Report\n",
        "report  = bibfile.eda_bib()\n",
        "\n",
        "# Check Report\n",
        "report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AJOsY2W_BoM"
      },
      "outputs": [],
      "source": [
        "# Check Docs IDs\n",
        "data_table.DataTable(bibfile.table_id_doc, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sgz81QTf_BoM"
      },
      "outputs": [],
      "source": [
        "# Check Docs IDs per Type\n",
        "data_table.DataTable(bibfile.id_doc_types(), num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyjG-IRf_BoM"
      },
      "outputs": [],
      "source": [
        "# Check Authors IDs\n",
        "data_table.DataTable(bibfile.table_id_aut, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpfZQaSl_BoN"
      },
      "outputs": [],
      "source": [
        "# Check Sources IDs\n",
        "data_table.DataTable(bibfile.table_id_jou, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNogO6Xs_BoN"
      },
      "outputs": [],
      "source": [
        "# Check Institutions IDs\n",
        "data_table.DataTable(bibfile.table_id_uni, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXdCo57A_BoO"
      },
      "outputs": [],
      "source": [
        "# Check Countries IDs\n",
        "data_table.DataTable(bibfile.table_id_ctr, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh2p-5Lz_BoO"
      },
      "outputs": [],
      "source": [
        "# Check Authors Keywords IDs\n",
        "data_table.DataTable(bibfile.table_id_kwa, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Htt7Oln8_BoO"
      },
      "outputs": [],
      "source": [
        "# Check Keywords Plus IDs\n",
        "data_table.DataTable(bibfile.table_id_kwp, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiQdiGsN_BoO"
      },
      "outputs": [],
      "source": [
        "# WordCloud from the Abstracts, Title, Authors Keywords or Keywords Plus\n",
        "# Arguments: entry = 'abs', 'title', 'kwa', or 'kwp'\n",
        "bibfile.word_cloud_plot(entry = 'abs', size_x = 15, size_y = 10, wordsn = 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFMg0Hoy4pFV"
      },
      "outputs": [],
      "source": [
        "# Check Table\n",
        "table             = PrettyTable()\n",
        "data_wd           = bibfile.ask_gpt_wd\n",
        "table.field_names = ['Word', 'Importance']\n",
        "for key, value in data_wd.items():\n",
        "    table.add_row([key, round(value, 4)])\n",
        "print(table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U82tvIvb_BoP"
      },
      "outputs": [],
      "source": [
        "# N-Grams\n",
        "# Arguments: view       = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            entry      = 'abs', 'title', 'kwa', or 'kwp'\n",
        "#            n_grams    = An integer with size n (representing the most common groups of words with size n)\n",
        "#            stop_words = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
        "#                         'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
        "#                         'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Potuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
        "#                         'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainian\n",
        "#            rmv_custom_words  = A list of custom stopwords to clean the corpus\n",
        "#             wordsn           = Number of N-Grams\n",
        "bibfile.get_top_ngrams(view = 'notebook', entry = 'kwp', ngrams = 3, stop_words = [], rmv_custom_words = [], wordsn = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FaapNGV5bM_"
      },
      "outputs": [],
      "source": [
        "# Check Table\n",
        "data_ng = bibfile.ask_gpt_ng\n",
        "data_table.DataTable(data_ng, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkiiRftr_BoQ"
      },
      "outputs": [],
      "source": [
        "# Documents Projection based on Words. (An interactive plot). It returns the Projection (each document coordinate) and the Labels (each document cluster)\n",
        "# Arguments: view              = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            corpus_type       = 'abs', 'title', 'kwa', or 'kwp';\n",
        "#            stop_words        = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
        "#                                'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
        "#                                'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Potuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
        "#                                'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainian\n",
        "#            rmv_custom_words  = A list of custom stopwords to clean the corpus;\n",
        "#            custom_label      = A list of custom labels for each document. The user can define each document cluster;\n",
        "#            custom_projection = A list of custom coordinates for each document. The user can define each document coordinate;\n",
        "#            n_components      = Number of Dimensions;\n",
        "#            n_clusters        = Number of Clusters;\n",
        "#            tf_idf            = True or False (True -> The Cluster Algorithm will use the DTM to calculate each document Label. False -> The Cluster Algorithm will use the Coordinates to calculate each document Label)\n",
        "#            embeddings        = True or False (True -> The Cluster Algorithm will use the Word Embeddings to calculate each document Label. False -> The Cluster Algorithm will use the Coordinates to calculate each document Label)\n",
        "#            method            = 'tsvd' or 'umap' ('tsvd' -> Truncated SVD projection method is used. 'umap' -> UMAP projection method is used)\n",
        "projection, labels = bibfile.docs_projection(view              = 'notebook',\n",
        "                                             corpus_type       = 'abs',\n",
        "                                             stop_words        = ['en'],\n",
        "                                             rmv_custom_words  = [],\n",
        "                                             custom_label      = [],\n",
        "                                             custom_projection = [],\n",
        "                                             n_components      = 2,\n",
        "                                             n_clusters        = 5,\n",
        "                                             tf_idf            = False,\n",
        "                                             embeddings        = False,\n",
        "                                             method            = 'umap')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywT35wBe54q4"
      },
      "outputs": [],
      "source": [
        "# Check Table\n",
        "data_pr = pd.DataFrame(np.hstack([projection, labels.reshape(-1,1)]))\n",
        "data_table.DataTable(data_pr, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCt6nL7r_BoR"
      },
      "outputs": [],
      "source": [
        "# Check Articles per Cluster\n",
        "cluster      = 0\n",
        "idx_articles = [i for i in range(0, labels.shape[0]) if labels[i] == cluster]\n",
        "print(*idx_articles, sep = ', ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ3JjjKp_BoR"
      },
      "outputs": [],
      "source": [
        "# Arguments: view              = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            key               = 'abs', 'title', 'jou, 'kwa', or 'kwp';\n",
        "#            stop_words        = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
        "#                                'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
        "#                                'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Potuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
        "#                                'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainian\n",
        "#            rmv_custom_words  = A list of custom stopwords to clean the corpus;\n",
        "#            topn              = Total number entities;\n",
        "#            start             = Start Year; -1 = all years\n",
        "#            end               = End Year;   -1 = all years\n",
        "bibfile.plot_evolution_year(view             = 'notebook',\n",
        "                            stop_words       = ['en'],\n",
        "                            rmv_custom_words = [],\n",
        "                            key              = 'title',\n",
        "                            topn             = 8,\n",
        "                            start            = 2000,\n",
        "                            end              = 2023)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R6XmiHW7nz1"
      },
      "outputs": [],
      "source": [
        "# View Table\n",
        "data_ep = bibfile.ask_gpt_ep\n",
        "print(textwrap.fill(data_ep, 150))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuUYvtG2_BoS"
      },
      "outputs": [],
      "source": [
        "# Sankey Diagram (An interactive plot)\n",
        "# Arguments: view  = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            entry = a list of any length of the following keys -> 'aut', 'cout', 'inst', 'jou', 'kwa', 'kwp', 'lan';\n",
        "#            topn  = Total number entities\n",
        "bibfile.sankey_diagram(view = 'notebook', entry = ['aut', 'cout', 'inst', 'lan'], topn = 10)\n",
        "\n",
        "# PS: The white bars can be dragged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsTaS_h18Gzz"
      },
      "outputs": [],
      "source": [
        "# View Table\n",
        "data_sk = bibfile.ask_gpt_sk\n",
        "data_table.DataTable(data_sk, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQZUFDvF_BoT"
      },
      "outputs": [],
      "source": [
        "# Tree Map\n",
        "# Arguments: entry = 'kwp', 'kwa', 'aut', 'jou', 'ctr', or 'inst';\n",
        "#            topn  = Total number entities\n",
        "bibfile.tree_map(entry = 'jou', topn = 20, size_x = 30, size_y = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP_DGzqT_BoT"
      },
      "outputs": [],
      "source": [
        "# Authors Productivity Plot (An interactive plot). It informs for each year the documents (IDs) published for each author\n",
        "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            topn = Total number entities\n",
        "bibfile.authors_productivity(view = 'notebook', topn = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqYGKZOH8Zv7"
      },
      "outputs": [],
      "source": [
        "# View Table\n",
        "data_ap = bibfile.ask_gpt_ap\n",
        "data_table.DataTable(data_ap, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZrovn05_BoU"
      },
      "outputs": [],
      "source": [
        "# Bar Plots\n",
        "# Arguments: statistic = 'dpy', 'cpy', 'ppy', 'ltk', 'spd', 'spc', 'apd', 'apc', 'aph', 'bdf_1', 'bdf_2', 'bdf_3', 'ipd', 'ipc', 'cpd', 'cpc', 'lpd', 'kpd', 'kad'\n",
        "#                        'dpy' = Documents per Year\n",
        "#                         cpy' = Citations per Year\n",
        "#                        'ppy' = Past Citations per Year\n",
        "#                        'ltk' = Lotka's Law\n",
        "#                        'spd' = Sources per Documents\n",
        "#                        'spc' = Sources per Citations\n",
        "#                        'apd' = Authors per Documents\n",
        "#                        'apc' = Authors per Citations\n",
        "#                        'aph' = Authors per H-Index\n",
        "#                        'bdf_1', 'bdf_2', 'bdf_3' = Bradford's Law - Core Sources 1, 2 or 3\n",
        "#                        'ipd' = Institutions per Documents\n",
        "#                        'ipc' = Institutions per Citations\n",
        "#                        'cpd' = Countries per Documents\n",
        "#                        'cpc' = Countries per Citations\n",
        "#                        'lpd' = Language per Documents\n",
        "#                        'kpd' = Keywords Plus per Documents\n",
        "#                        'kad' = Authors' Keywords per Documents\n",
        "#            topn      = Total number entities\n",
        "bibfile.plot_bars(statistic = 'apd', topn = 20, size_x = 15, size_y = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxihXvY08zE5"
      },
      "outputs": [],
      "source": [
        "# View Table\n",
        "data_bp = bibfile.ask_gpt_bp\n",
        "data_table.DataTable(data_bp, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network analysis"
      ],
      "metadata": {
        "id": "dc9zVDOunM0T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxVsdp_E_BoV"
      },
      "outputs": [],
      "source": [
        "# Network - Citation Analisys Between Documents (Blue Nodes) and Citations (Red Nodes).  (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            min_count   = Relationship between nodes that have been cited at least x times;\n",
        "#            node_labels = True or False (True -> The label IDs will be displayed, False -> Only the nodes will be displayed );\n",
        "#            local_nodes = True or False (True -> Only the blue will be displayed, False -> Red and Blue nodes will be displayed)\n",
        "bibfile.network_adj_dir(view = 'notebook', min_count = 7, node_labels = True, local_nodes = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PydX9A7v9RSK"
      },
      "outputs": [],
      "source": [
        "# View Table\n",
        "data_nad = bibfile.ask_gpt_nad\n",
        "data_table.DataTable(data_nad, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8bCgM0L_BoV"
      },
      "outputs": [],
      "source": [
        "# Network - Highlight Citation Analysis Between Documents (Blue Nodes) and Citations (Red Nodes).  (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            article_ids = A list of Blue Nodes. It indicates the documents cited by them;\n",
        "#            ref_ids     = A list of Red Nodes. It indicates the documents that cites them\n",
        "bibfile.find_nodes_dir(view = 'notebook', article_ids = ['44', '235'], ref_ids = [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEgzigSL_BoW"
      },
      "outputs": [],
      "source": [
        "# Network - Highlight Citation Analysis Between Documents (Blue Nodes) and Citations (Red Nodes).  (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            article_ids = A list of Blue Nodes. It indicates the documents cited by them;\n",
        "#            ref_ids     = A list of Red Nodes. It indicates the documents that cites them\n",
        "bibfile.find_nodes_dir(view = 'notebook', article_ids = [], ref_ids = ['r_5605'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8O5EjQp_BoW"
      },
      "outputs": [],
      "source": [
        "# Network - Local Documents (Only Blue Nodes) Citation History. (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            min_count   = Relationship between nodes that have connected at least x times;\n",
        "#            node_size   = -1. (If node_size = -1 then the default value will be used. If node_size > 0 then this new value will be used);\n",
        "#            node_labels = True or False (True -> The label IDs will be displayed, False -> Only the nodes will be displayed );\n",
        "#            back        = A list of documents. It shows the documents cited by them direct and indirectly;\n",
        "#            forward     = A list of documents. It shows the documents that cites them direct and indirectly\n",
        "bibfile.network_hist(view = 'notebook', min_count = 0, node_size = -1, node_labels = True, back = [], forward = [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzEx_ewz9mBa"
      },
      "outputs": [],
      "source": [
        "# View Table\n",
        "data_hist = bibfile.ask_gpt_hist\n",
        "data_table.DataTable(data_hist, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVLRl1mT_BoX"
      },
      "outputs": [],
      "source": [
        "# Network - Local Documents (Only Blue Nodes) Citation History. (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            min_count   = Relationship between nodes that have connected at least x times;\n",
        "#            node_size   = -1. (If node_size = -1 then the default value will be used. If node_size > 0 then this new value will be used);\n",
        "#            node_labels = True or False (True -> The label IDs will be displayed, False -> Only the nodes will be displayed );\n",
        "#            back        = A list of documents. It shows the documents cited by them direct and indirectly;\n",
        "#            forward     = A list of documents. It shows the documents that cites them direct and indirectly\n",
        "bibfile.network_hist(view = 'notebook', min_count = 0, node_size = -1, node_labels = True, back = ['24'], forward = [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUtq8EEQ_BoX"
      },
      "outputs": [],
      "source": [
        "# Network - Local Documents (Only Blue Nodes) Citation History. (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            min_count   = Relationship between nodes that have connected at least x times;\n",
        "#            node_size   = -1. (If node_size = -1 then the default value will be used. If node_size > 0 then this new value will be used);\n",
        "#            node_labels = True or False (True -> The label IDs will be displayed, False -> Only the nodes will be displayed );\n",
        "#            back        = A list of documents. It shows the documents cited by them direct and indirectly;\n",
        "#            forward     = A list of documents. It shows the documents that cites them direct and indirectly\n",
        "bibfile.network_hist(view = 'notebook', min_count = 0, node_size = -1, node_labels = True, back = [], forward = ['97'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0hMpoQl_BoX"
      },
      "outputs": [],
      "source": [
        "# Network - Collaboration Analysis Between Authors, Countries, Intitutions Or Adjacency Analysis Between Authors' Keywords or Keywords Plus. (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            adj_type    = 'aut', 'cout', 'inst', 'kwa', or 'kwp'\n",
        "#            min_count   = Relationship between nodes that have connected at least x times;\n",
        "#            node_labels = True or False (True -> The label IDs will be displayed, False -> Only the nodes will be displayed );\n",
        "#            node_size   = -1. (If node_size = -1 then the default value will be used. If node_size > 0 then this new value will be used);\n",
        "#            label_type  = 'id', 'name' (Only meaningfull if node_labels = True. 'id' -> The ID will be displayed; 'name' -> The name will be displayed);\n",
        "#            centrality  = 'degree', 'load', 'betw', 'close', 'eigen', 'katz', 'harmonic', or None. Color nodes according to centrality criterion\n",
        "#                          'degree'   = Degree Centrality\n",
        "#                          'load'     = Load Centrality\n",
        "#                          'betw'     = Betweenness Centrality\n",
        "#                          'close'    = Closeness Centrality\n",
        "#                          'eigen'    = Eigenvector Centrality\n",
        "#                          'katz'     = Katz Centrality\n",
        "#                          'harmonic' = Harmonic Centrality\n",
        "#                           None      = The Community Algorithm, Girvan-Newman, will be used Instead of a Centrality Criterion\n",
        "bibfile.network_adj(view = 'notebook', adj_type = 'aut', min_count = 5, node_labels = True, label_type = 'name', centrality = None)\n",
        "\n",
        "# PS: If a centrality criterion is used then the values can be obtained by the following command:  bibfile.table_centr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYLWm2DJ-C4f"
      },
      "outputs": [],
      "source": [
        "# View Table\n",
        "data_adj = bibfile.ask_gpt_adj\n",
        "data_table.DataTable(data_adj, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp1yJdQ__BoY"
      },
      "outputs": [],
      "source": [
        "# Network - Highlight  Collaboration Analysis Between Authors, Countries, Intitutions Or Adjacency Analysis Between Authors' Keywords or Keywords Plus. (An interactive plot).\n",
        "# Arguments: view      = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            node_ids  = A list of IDs. Only meaningfull if label_type = 'id';\n",
        "#            node_name = A list of Names. Only meaningfull iflabel_type = 'name';\n",
        "#            node_only = True or False (True -> Only the Node will be Highlighted, False -> Node and its Connections will be Highlighted)\n",
        "bibfile.find_nodes(node_ids = [], node_name = ['figueira, j.r.'], node_only = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1_c3XYL_BoY"
      },
      "outputs": [],
      "source": [
        "# Network - Similarity Analysis using coupling or cocitation methods. (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            sim_type    = 'coup', 'cocit' ('coup' -> Coupling Method, 'cocit' -> Cocitation Method)\n",
        "#            node_size   = -1. (If node_size = -1 then the default value will be used. If node_size > 0 then this new value will be used);\n",
        "#            node_labels = True or False (True -> The label IDs will be displayed, False -> Only the nodes will be displayed );\n",
        "#            cut_coup    = Cutoff value for Coupling Method. Only meaninfull if sim_type = 'coup';\n",
        "#            cut_cocit   = Cutoff value for Cocitation Method. Only meaninfull if sim_type = 'cocit'\n",
        "bibfile.network_sim(view = 'notebook', sim_type = 'cocit', node_size = 10, node_labels = True, cut_coup = 0.3, cut_cocit = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUHMIatj_BoZ"
      },
      "outputs": [],
      "source": [
        "# Check Similarity Values\n",
        "data_table.DataTable(bibfile.sim_table, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8dnxtAk_BoZ"
      },
      "outputs": [],
      "source": [
        "# Network - Collaboration Analysis Between Countries using a Map. (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            connections = True or False (True -> Countries connections will be displayed, False -> Countries connections will not be displayed);\n",
        "#            country_lst = Highlight the Connections Between a List of Countries\n",
        "bibfile.network_adj_map(view = 'notebook', connections = True, country_lst = [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC_Fyg7z-XDE"
      },
      "outputs": [],
      "source": [
        "# View Table\n",
        "data_map = bibfile.ask_gpt_map\n",
        "data_table.DataTable(data_map, num_rows_per_page = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoWPjO1Cu2ic"
      },
      "outputs": [],
      "source": [
        "# Network - Collaboration Analysis Between Countries using a Map. (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            connections = True or False (True -> Countries connections will be displayed, False -> Countries connections will not be displayed);\n",
        "#            country_lst = Highlight the Connections Between a List of Countries\n",
        "bibfile.network_adj_map(view = 'notebook', connections = False, country_lst = ['Mexico'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Litstudy"
      ],
      "metadata": {
        "id": "4d1HsdYcpdsR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8F8-25p31R-"
      },
      "outputs": [],
      "source": [
        "litstudy.plot_year_histogram(docs_SS, vertical=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNMLH0U14P_a"
      },
      "outputs": [],
      "source": [
        "litstudy.plot_cocitation_network(docs_SS, max_edges=500)\n",
        "# I think this does not work, as I have not exported all the records including papers citing other papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FO7l4OM4Y6b"
      },
      "outputs": [],
      "source": [
        "corpus = litstudy.build_corpus(docs_SS, ngram_threshold=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMkLrFhL4eyo"
      },
      "outputs": [],
      "source": [
        "litstudy.compute_word_distribution(corpus).filter(like='_', axis=0).sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az_YlIymjnKy"
      },
      "outputs": [],
      "source": [
        "litstudy.compute_word_distribution(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lByIaR-j4ney"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 3))\n",
        "litstudy.plot_word_distribution(corpus, limit=50, title=\"Top words\", vertical=True, label_rotation=45);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Processing and Topic modelling"
      ],
      "metadata": {
        "id": "j0yhDX76nS-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyBibx"
      ],
      "metadata": {
        "id": "JdFUoovxqEtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic"
      ],
      "metadata": {
        "id": "lJA39aRVCdU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = BERTopic(verbose=True)\n",
        "topics, _ = topic_model.fit_transform(docs_SS)"
      ],
      "metadata": {
        "id": "AOBBBUi6X8vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "docs = fetch_20newsgroups(subset='all', remove=('headers','footers', 'quotes'))['data']\n",
        "topic_model = BERTopic(calculate_probabilities=True)\n",
        "topics, probs = topic_model.fit_transform(docs)"
      ],
      "metadata": {
        "id": "YQvWcKEaYWxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DQydEMp-AdH"
      },
      "outputs": [],
      "source": [
        "# NLP\n",
        "# Arguments: corpus_type       = 'abs', 'title', 'kwa', or 'kwp';\n",
        "#            stop_words        = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
        "#                                'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
        "#                                'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Potuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
        "#                                'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainian\n",
        "#            rmv_custom_words  = A list of custom stopwords to clean the corpus;\n",
        "bibfile.create_embeddings(stop_words = ['en'], rmv_custom_words = [], corpus_type = 'abs')\n",
        "emb = bibfile.embds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm13b3U9Cgz_"
      },
      "outputs": [],
      "source": [
        "# NLP #-1 refers to all outliers and should typically be ignored.\n",
        "# Arguments: stop_words        = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
        "#                              'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
        "#                              'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Potuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
        "#                              'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainianian;   'es' =  Spanish;  'sv' = Swedish\n",
        "#            rmv_custom_words  = A list of custom stopwords to clean the corpus;\n",
        "#            embeddings        = True or False. If True then word embeddings are used to create the topics\n",
        "bibfile.topics_creation(stop_words = ['en'], rmv_custom_words = ['agriculture', 'technologies', 'technology', 'agricultural', 'industry'], embeddings = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKnung64CseX"
      },
      "outputs": [],
      "source": [
        "# NLP\n",
        "# Each document Topic\n",
        "topics = bibfile.topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skh6EMP8DGo7"
      },
      "outputs": [],
      "source": [
        "# NLP\n",
        "# Each document Probability to belong a Topic\n",
        "probs = bibfile.probs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bibfile.graph_topics(view = 'notebook')"
      ],
      "metadata": {
        "id": "P9eQy8UBetOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsAAbo2eDQ4o"
      },
      "outputs": [],
      "source": [
        "# NLP\n",
        "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window)\n",
        "bibfile.graph_topics_distribution(view = 'notebook')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDj33Pi1DnEs"
      },
      "outputs": [],
      "source": [
        "# NLP\n",
        "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window)\n",
        "bibfile.graph_topics(view = 'notebook')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpLx6z12DsPI"
      },
      "outputs": [],
      "source": [
        "# NLP\n",
        "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window)\n",
        "bibfile.graph_topics_projection(view = 'notebook')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytTGEDBuDuwQ"
      },
      "outputs": [],
      "source": [
        "# NLP\n",
        "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window)\n",
        "bibfile.graph_topics_heatmap(view = 'notebook')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFRl-a5dF51Z"
      },
      "outputs": [],
      "source": [
        "# NLP\n",
        "bibfile.topics_representatives()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi-qtpKUGaZ3"
      },
      "outputs": [],
      "source": [
        "# NLP\n",
        "similar_topics, similarity = bibfile.topic_model.find_topics('electre', top_n = 10)\n",
        "for i in range(0, len(similar_topics)):\n",
        "  print('Topic: ', similar_topics[i], 'Correlation: ', round(similarity[i], 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlKvyTO_Hrd7"
      },
      "outputs": [],
      "source": [
        "# NLP\n",
        "bibfile.topic_model.save('my_topic_model')\n",
        "#loaded_topic_model = BERTopic.load('my_topic_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBYNqxljhSjK"
      },
      "outputs": [],
      "source": [
        "# NLP - Abstractive Summarization\n",
        "# Arguments: article_ids = A list of documents to perform an abstractive summarization with the available abstracts. If the list is empty then all documents will be used\n",
        "#            model_name  = Available pre-trained models. Complete list is available at  https://huggingface.co/models?pipeline_tag=summarization&sort=downloads&search=pegasus\n",
        "abs_summary = bibfile.summarize_abst_peg(article_ids = [218, 28, 212], model_name = 'google/pegasus-xsum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_ugzx3sH1iZ"
      },
      "outputs": [],
      "source": [
        "# NLP - Check Abstractive Summarization\n",
        "print(textwrap.fill(abs_summary, 150))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuIyocQkWpsF"
      },
      "outputs": [],
      "source": [
        "# NLP - Abstractive Summarization - chatGPT\n",
        "\n",
        "# OBS 1: Requires the user to have an **API key** (https://platform.openai.com/account/api-keys))\n",
        "# OBS 2: The limit of characters is 4097 per request\n",
        "\n",
        "# Arguments: article_ids   = A list of documents to perform an abstractive summarization with the available abstracts. If the list is empty then all documents will be used\n",
        "#            join_articles = If False then the abstracts will be analyzed separately. If True then the abstracts will be concate in a single text\n",
        "#            api_key       = 'your_api_key_here'. Insert your personal API key (https://platform.openai.com/account/api-keys)\n",
        "#            model         = Specifies the AI model used for text generation. The default value is \"text-davinci-003\"\n",
        "#            query         = Ask chatGPT what you want to do with the abstracts. The default query is: 'from the following scientific abstracts, summarize the main information in a single paragraph using around 250 words'\n",
        "abs_summary_chat = bibfile.summarize_abst_chatgpt(article_ids = [218, 28, 212], join_articles = True, api_key = 'your_api_key_here', query = 'from the following scientific abstracts, summarize the main information in a single paragraph using around 250 words', model = 'gpt-4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QblNnsZFXZ5c"
      },
      "outputs": [],
      "source": [
        "# NLP - Check Abstractive Summarization\n",
        "print(textwrap.fill(abs_summary_chat, 250))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aywSSF4BH1sS"
      },
      "outputs": [],
      "source": [
        "# NLP - Extractive Summarization\n",
        "# Arguments: article_ids = A list of documents to perform an extractive summarization with the available abstracts. If the list is empty then all documents will be used\n",
        "ext_summary = bibfile.summarize_ext_bert(article_ids = [218, 28, 212])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qW_Ek64hd8B"
      },
      "outputs": [],
      "source": [
        "# NLP - Check Extractive Summarization\n",
        "print(textwrap.fill(ext_summary, 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx74Fohnjk3x"
      },
      "outputs": [],
      "source": [
        "# Filter the .bib File\n",
        "# Arguments: document = A list of documents to retain. The other documents will be deleted\n",
        "#            doc_type = A list of doc types. Check the 'report' to select the available types;\n",
        "#            year_str = An integer the determines the starting year of collection -1 = All years;\n",
        "#            year_end = An integer the determines the ending year of collection   -1 = All years;\n",
        "#            sources  = A list of sources. Check the cell '# Check Sources IDs' to select the available types;\n",
        "#            core     = A integer (-1, 1, 2, 3, 12, or 23) -1 = All sources, 1 = Bradford core 1, 2 = Bradford core 2, 3 = Bradford core 3, 12 = Bradford core 1 and 2, 23 = Bradford core 2 and 3;\n",
        "#            country  = A list of countries. Check the cell '# Check Countries IDs' to select the available types;\n",
        "#            language = A list of languages. Check the 'report' to select the available types\n",
        "#            abstract = True or False. True removes UNKNOW values from the abstract.\n",
        "bibfile.filter_bib(documents = [], doc_type = [], year_str = -1, year_end = -1, sources = [], core = -1, country = [], language = [], abstract = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuQ854UdlzAt"
      },
      "outputs": [],
      "source": [
        "# Correct the .bib File\n",
        "# Arguments: get         = A list of the current name(s);\n",
        "#            replace_for = A string. This string will replace all matchs from the 'get' argument list\n",
        "bibfile.merge_author(get = [], replace_for = 'name')\n",
        "bibfile.merge_institution(get = [], replace_for = 'name')\n",
        "bibfile.merge_country(get = [], replace_for = 'name')\n",
        "bibfile.merge_language(get = [], replace_for = 'name')\n",
        "bibfile.merge_source(get = [], replace_for = 'name')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lOkkNMOL9SN"
      },
      "source": [
        "Litstudy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8I0X1ipLE0W"
      },
      "outputs": [],
      "source": [
        "num_topics = 20\n",
        "topic_model = litstudy.train_nmf_model(corpus, num_topics, max_iter=250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6huK13lPLOA0"
      },
      "outputs": [],
      "source": [
        "for i in range(num_topics):\n",
        "    print(f'Topic {i+1}:', topic_model.best_tokens_for_topic(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU-e3ikCLPNM"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "litstudy.plot_topic_clouds(topic_model, ncols=5);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F4GbjxwLWkL"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "litstudy.plot_embedding(corpus, topic_model);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SShBOC3cLxA-"
      },
      "outputs": [],
      "source": [
        "topic_id = topic_model.best_topic_for_token('pef')\n",
        "print(topic_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9bdhsisLx59"
      },
      "outputs": [],
      "source": [
        "for doc_id in topic_model.best_documents_for_topic(topic_id, limit=10):\n",
        "    print(docs_SS[int(doc_id)].title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZQ5qobCL2Ql"
      },
      "outputs": [],
      "source": [
        "threshold = 0.2\n",
        "dl_topic = topic_model.doc2topic[:, topic_id] > threshold\n",
        "print(dl_topic, sep = \"\\n\")\n",
        "n=len(dl_topic)\n",
        "print(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJzjloFEZaIE"
      },
      "outputs": [],
      "source": [
        "docs = docs_SS.add_property('dl_topic', dl_topic)\n",
        "\n",
        "groups = {\n",
        "    'deep learning related': 'dl_topic',\n",
        "    'other': 'not dl_topic',\n",
        "}\n",
        "\n",
        "litstudy.plot_year_histogram(docs, groups=groups, stacked=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VXxByZDL3iJ"
      },
      "outputs": [],
      "source": [
        "table = litstudy.compute_year_histogram(docs_SS, groups=groups)\n",
        "table.div(table.sum(axis=1), axis=0) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_BW8o7oL8jw"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "litstudy.plot_source_histogram(docs, groups=groups, limit=25, stacked=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhU_YQDtL_aM"
      },
      "outputs": [],
      "source": [
        "# Compute histogram by publication venue\n",
        "table = litstudy.compute_source_histogram(docs, groups=groups)\n",
        "\n",
        "# Add column 'total'\n",
        "table['total'] = table['deep learning related'] + table['other']\n",
        "\n",
        "# Remove rare venues that have less than 5 publications\n",
        "table = table[table['total'] >= 5]\n",
        "\n",
        "# Add column 'ratio'\n",
        "table['ratio'] = table['deep learning related'] / table['total'] * 100\n",
        "\n",
        "# Sort by ratio in descending order\n",
        "table.sort_values(by='ratio', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bibliometrix implementation from R"
      ],
      "metadata": {
        "id": "8EMizD3wGgXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "id": "jeipUi9YCB8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TenxuGW9paw"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "install.packages(\"bibliometrix\", dependencies=TRUE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "library(bibliometrix)"
      ],
      "metadata": {
        "id": "lv4c8eUdEKQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "file <- \"/content/drive/MyDrive/MEM RA data/4IR.bib\"\n",
        "\n",
        "M <- convert2df(file = file, dbsource = \"scopus\", format = \"bibtex\")"
      ],
      "metadata": {
        "id": "GRel-hbsFiE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "results <- biblioAnalysis(M, sep = \";\")"
      ],
      "metadata": {
        "id": "Hb3AmXI5CH34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "options(width=100)\n",
        "S <- summary(object = results, k = 10, pause = FALSE)"
      ],
      "metadata": {
        "id": "tQpBHE9fH9Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "plot(x = results, k = 10, pause = FALSE)"
      ],
      "metadata": {
        "id": "B6RDXHgkI19I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "M$CR[1]"
      ],
      "metadata": {
        "id": "glqnGDnKJidn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Create keyword co-occurrences network\n",
        "\n",
        "NetMatrix <- biblioNetwork(M, analysis = \"co-occurrences\", network = \"keywords\", sep = \";\")\n",
        "\n",
        "# Plot the network\n",
        "net=networkPlot(NetMatrix, normalize=\"association\", weighted=T, n = 50, Title = \"Keyword Co-occurrences\", type = \"fruchterman\", size=T,edgesize = 10,labelsize=0.8)"
      ],
      "metadata": {
        "id": "jNp8t5YtQt65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Conceptual Structure using keywords (method=\"CA\")\n",
        "\n",
        "CS <- conceptualStructure(M,field=\"DE\", method=\"MCA\", minDegree=4, clust=8, stemming=FALSE, labelsize=10, documents=50)"
      ],
      "metadata": {
        "id": "8igoHzDjR7-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "res <- thematicMap(M, field = \"ID\", n = 250, minfreq = 5, ngrams = 1, stemming = FALSE, size = 0.5, n.labels = 1, community.repulsion = 0.1,\n",
        "repel = TRUE, remove.terms = NULL, synonyms = NULL, cluster = \"walktrap\", subgraphs = FALSE)\n",
        "plot(res$map)"
      ],
      "metadata": {
        "id": "KHsxMr1tXfWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "res <- thematicEvolution(M, field = \"ID\", years = c(2018,2020,2022), n = 200, minFreq = 2,size = 0.2, ngrams = 1,\n",
        "stemming = FALSE, n.labels = 1, repel = TRUE, remove.terms = NULL, synonyms = NULL, cluster = \"leiden\")\n",
        "plot(res$map)"
      ],
      "metadata": {
        "id": "1QZM6hMbaCaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa22SdM2E2x4"
      },
      "source": [
        "This notebook is based on Maarten Grootendorst [BERTopic](https://github.com/MaartenGr/BERTopic/tree/v0.4.2) tutorial available [here](https://github.com/MaartenGr/BERTopic/blob/v0.4.2/notebooks/BERTopic.ipynb) and the implementation by Abuayed [here](https://github.com/iwan-rg/Arabic-Topic-Modeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXHLDxJdRzBi"
      },
      "source": [
        "# **BERT for Arabic Topic Modeling: An Experimental Study on BERTopic Technique**\n",
        "Abeer Abuzayed and Hend Al-Khalifa\n",
        "\n",
        "# Abstract\n",
        "Topic modeling is an unsupervised machine learning technique for finding abstract topics in a large collection of documents. It\n",
        "helps in organizing, understanding and summarizing large collections of textual information and discovering the latent topics that\n",
        "vary among documents in a given corpus. Latent Dirichlet allocation (LDA) and Non-Negative Matrix Factorization (NMF) are\n",
        "two of the most popular topic modeling techniques. LDA uses a probabilistic approach whereas NMF uses matrix factorization\n",
        "approach, however, new techniques that are based on BERT for topic modeling do exist. In this paper, we aim to experiment with\n",
        "BERTopic using different Pre-Trained Arabic Language Models as embeddings, and compare its results against LDA and NMF\n",
        "techniques. We used Normalized Pointwise Mutual Information (NPMI) measure to evaluate the results of topic modeling\n",
        "techniques. The overall results generated by BERTopic showed better results compared to NMF and LDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYKCnnwdEsLb"
      },
      "source": [
        "**NOTE**: Make sure to select a GPU runtime. Otherwise, the model can take quite some time to create the document embeddings!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNa-KtKDRnus"
      },
      "source": [
        "# we start with installing bertopic from pypi before preparing the data\n",
        "\n",
        "!pip install bertopic[all]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGjZitDgGKup"
      },
      "source": [
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import LdaMulticore\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyd1PrqeHI5O"
      },
      "source": [
        "# Load Data\n",
        "For this experiment, We used [(DataSet for Arabic Classification)](https://data.mendeley.com/datasets/v524p5dhpj/2) which contains 111,728 Arabic documents written in Modern\n",
        "Standard Arabic (MSA). The dataset was collected from three Arabic online newspapers: Assabah, Hespress and\n",
        "Akhbarona. The documents in the dataset are categorized into 5 classes: sport, politics, culture, economy and diverse.\n",
        "We removed 2939 missing documents and ran the experiments with the remaining 108789 documents without any\n",
        "document labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4D7RY36GF0L"
      },
      "source": [
        "# add your data path\n",
        "\n",
        "data=  pd.read_csv(\"xxx.csv\")\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5Znx7MIf4UX"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySSAB2KzIADz"
      },
      "source": [
        "data=data.dropna()\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S6Q96ZmGZhr"
      },
      "source": [
        "documents = data['text'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2SRCMaPIizW"
      },
      "source": [
        "#Embedding model\n",
        "BERTopic has two default embedding models: \"distilbert-base-nli-stsb-mean-tokens'' for the English language and \"xlm-r-bert-base-nli-stsb-meantokens\" for any language other than English, where XLM-R models support 50+ languages.\n",
        "\n",
        "Also, you can select any model from [Hugging Face](https://huggingface.co/models)  and use it instead of the preselected models by simply passing the model through\n",
        "BERTopic with embedding_model.\n",
        "\n",
        "For more deatelis check out BERTopic decomntion [here](https://maartengr.github.io/BERTopic/tutorial/embeddings/embeddings.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCW-jMbyUffN"
      },
      "source": [
        "#to experiment with other BERT models simply change the model name below\n",
        "\n",
        "arabert = TransformerDocumentEmbeddings('aubmindlab/bert-base-arabertv02')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBcNmZJzSTY8"
      },
      "source": [
        "# **Create Topics**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkbmf2rmhGZx"
      },
      "source": [
        "For BERTopic you do not need to define the number of topics in advance, however, if you want to do so simply pass the number of topics to BERTopic with nr_topics paramete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGiQB1iC8pFl"
      },
      "source": [
        "topic_model = BERTopic(language=\"arabic\", low_memory=True ,calculate_probabilities=False,\n",
        "                     embedding_model=arabert)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIt8djAWe-OT"
      },
      "source": [
        "NOTE: Calculating probabilities can slow down BERTopic significantly at large amounts of data (>100_000 documents). It is advised to turn this off if you want to speed up the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfhfzqkoSJ1I"
      },
      "source": [
        "topics, probs = topic_model.fit_transform(documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiqknMl5yfqo"
      },
      "source": [
        "#extract most frequent topics\n",
        "\n",
        "topic_model.get_topic_freq().head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krD3o2zufHoV"
      },
      "source": [
        "-1 refers to all outliers and should typically be ignored. Next, let's take a look at the most frequent topic that was generated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Mu184PhuLK"
      },
      "source": [
        "#show the top 10 words in topic 1\n",
        "\n",
        "topic_model.get_topic(1)[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6uPNJbFYCjA"
      },
      "source": [
        "# Evaluation\n",
        "To evaluate the model topics coherence we use [Gensim](https://radimrehurek.com/gensim/models/coherencemodel.html) implementation of the Normalized\n",
        "Pointwise Mutual Information (NPMI)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxHtjF3q7Yo2"
      },
      "source": [
        "texts = [[word for word in str(document).split()] for document in documents]\n",
        "id2word = corpora.Dictionary(texts)\n",
        "corpus = [id2word.doc2bow(text) for text in texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHbifeYYv7rx"
      },
      "source": [
        "topics=[]\n",
        "for i in topic_model.get_topics():\n",
        "  row=[]\n",
        "  topic= topic_model.get_topic(i)\n",
        "  for word in topic:\n",
        "     row.append(word[0])\n",
        "  topics.append(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSxujGybrdMD"
      },
      "source": [
        "# compute Coherence Score\n",
        "\n",
        "cm = CoherenceModel(topics=topics, texts=texts, corpus=corpus, dictionary=id2word, coherence='c_npmi')\n",
        "coherence = cm.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8c8LenB8Zyl"
      },
      "source": [
        "# **Visualize Topics**\n",
        "After having trained our `BERTopic` model, we can iteratively go through perhaps a hundred topic to get a good\n",
        "understanding of the topics that were extract. However, that takes quite some time and lacks a global representation.\n",
        "Instead, we can visualize the topics that were generated in a way very similar to\n",
        "[LDAvis](https://github.com/cpsievert/LDAvis):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQKBcla28bY0"
      },
      "source": [
        "topic_model.visualize_topics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wekNoQNuUVoU"
      },
      "source": [
        "# **Model serialization**\n",
        "The model and its internal settings can easily be saved. Note that the documents and embeddings will not be saved. However, UMAP and HDBSCAN will be saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWUF1uxiSb_a"
      },
      "source": [
        "# Save model\n",
        "topic_model.save(\"my_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_eHBI1jSb6i"
      },
      "source": [
        "# Load model\n",
        "my_model = BERTopic.load(\"my_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UCb_fnUo_rJ"
      },
      "source": [
        "# LDA\n",
        "\n",
        "We use the [ parallelized Latent Dirichlet Allocation (LDA)](https://radimrehurek.com/gensim/models/ldamulticore.html) from Gensim.\n",
        "\n",
        "Note: for LDA you have to define topics number in advance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP89p9qGpA86"
      },
      "source": [
        "#chang the number of topics here\n",
        "no_topics = 5\n",
        "\n",
        "# run LDA\n",
        "lda = LdaMulticore(corpus, id2word=id2word, num_topics=no_topics)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep0Pl3qqpevJ"
      },
      "source": [
        "#compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda, texts=texts, dictionary=id2word, coherence='c_npmi')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAbf7WWupz6B"
      },
      "source": [
        "#NMF\n",
        "We use Scikit-learn implementation of [NMF](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html).\n",
        "\n",
        "Note: for NMF you have to define topics number in advance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ongX2jbZpifi"
      },
      "source": [
        "# NMF is able to use tf-idf\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
        "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE7N4UPUqlpf"
      },
      "source": [
        "#chang the number of topics here\n",
        "no_topics = 5\n",
        "\n",
        "# run NMF\n",
        "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Fm3Ox1q1WU"
      },
      "source": [
        "topics_NMF=[]\n",
        "for index, topic in enumerate(nmf.components_):\n",
        "    row=[]\n",
        "    for i in topic.argsort()[-10:]:\n",
        "      row.append(tfidf_vectorizer.get_feature_names()[i])\n",
        "    topics_NMF.append(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEzvoNFPrBft"
      },
      "source": [
        "cm = CoherenceModel(topics=topics_NMF, texts=texts, corpus=corpus, dictionary=id2word, coherence='c_npmi')\n",
        "coherence_nmf = cm.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_nmf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc8MJc0KteZS"
      },
      "source": [
        "If you use this notebook, please cite our paper :)\n",
        "\n",
        "```\n",
        "Abeer Abuzayed and Hend Al-Khalifa. BERT for Arabic Topic Modeling: An Experimental Study on BERTopic Technique. 5th International Conference on AI in Computational Linguistics, Procedia Computer Science (ISSN: 1877-0509), Elsevier, 2021. (in press).\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L26Gs35VW8ki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERTopic from Scopus by Daniele Atzeni\n",
        "from https://github.com/daniele-atzeni/A-Systematic-Review-of-Wi-Fi-and-Machine-Learning-Integration-with-Topic-Modeling-Techniques\n",
        "(forked to own repository)\n"
      ],
      "metadata": {
        "id": "n4cHV1BOLnEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "syWYi76PZzIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2JCTGRbXw3V"
      },
      "source": [
        "# BerTopic Atzeni Scopus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqV2Bv7biqGu"
      },
      "source": [
        "# IEEE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rwu7l_3ioOp"
      },
      "outputs": [],
      "source": [
        "ieee = pd.read_csv('raw_data/IEEE.csv')\n",
        "\n",
        "ieee.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zg20SkdOi1lt"
      },
      "outputs": [],
      "source": [
        "len(ieee)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xp7awIGHnNVA"
      },
      "outputs": [],
      "source": [
        "ieee['Document Identifier'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdYeWZ9CmD2B"
      },
      "outputs": [],
      "source": [
        "ieee.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrq0hD_ZoDrv"
      },
      "outputs": [],
      "source": [
        "ieee.iloc[0]['Authors'], scopus_tot.iloc[0]['Authors']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAQL9QiioC93"
      },
      "outputs": [],
      "source": [
        "# convert authors name as in Scopus\n",
        "def conv_auth_names_scop(x):\n",
        "    if type(x) != str:\n",
        "        return x\n",
        "    authors = x.split(';')\n",
        "    res = ''\n",
        "    for i, el in enumerate(authors):\n",
        "        last_space_idx = el.rfind(' ')\n",
        "        if i == 0:\n",
        "            new_auth = f'{el[last_space_idx:]} {el[:last_space_idx]}'   # there is no space before initials\n",
        "        else:\n",
        "            new_auth = el[last_space_idx:] + el[:last_space_idx]\n",
        "        res += new_auth\n",
        "        if i != len(authors) - 1:\n",
        "            res += ','  # additional space not needed\n",
        "    return res\n",
        "\n",
        "conv_auth_names_scop(ieee.iloc[0]['Authors'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dl6e3IgMp6s_"
      },
      "outputs": [],
      "source": [
        "ieee['Authors'] = ieee['Authors'].map(lambda x: conv_auth_names(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzUMUfZqmf7A"
      },
      "outputs": [],
      "source": [
        "# scopus rel columns ['Title', 'Year', 'Cited by', 'Abstract', 'Author Keywords', 'Authors', 'Document Type']\n",
        "ieee.rename(columns={'Document Title':'Title', 'Publication Year':'Year', 'Article Citation Count':'Cited by', 'Document Identifier':'Document Type'}, inplace=True)\n",
        "\n",
        "ieee = ieee[scopus_rel_columns]\n",
        "ieee.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn0cAS4CqOkq"
      },
      "source": [
        "# Drop Conferences and similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iIAxFSXr_pf"
      },
      "outputs": [],
      "source": [
        "df.sort_values(by='Title')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYaBKyjnqRaN"
      },
      "outputs": [],
      "source": [
        "def check_conference(title):\n",
        "    names = ['conference', 'workshop', 'symposium', 'meeting', 'forum']\n",
        "    for el in names:\n",
        "        if el in title:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "idxs = [not check_conference(title) for title in df['Title']]\n",
        "\n",
        "df = df.loc[idxs]\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0Xj2A-byhC9"
      },
      "outputs": [],
      "source": [
        "# fill N/A values and concatenate titles, abstracts, author kws\n",
        "df[['Title', 'Abstract', 'Author Keywords']] = df[['Title', 'Abstract', 'Author Keywords']].fillna('')\n",
        "df['Cited by'] = df['Cited by'].fillna(0)\n",
        "df['text'] = df['Title'] + ' ' + df['Abstract'] + ' ' + df['Author Keywords']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83X2v8ejzcpv"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpdDfpd_hgno"
      },
      "source": [
        "# Preprocess the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rKrkkolh5-p"
      },
      "outputs": [],
      "source": [
        "# custom stopwords have to be updated based on results - kept only some\n",
        "config = {\n",
        "    'remove_punct' : True,\n",
        "    'remove_num' : True,\n",
        "    'remove_stopwords' : True,\n",
        "    'custom_stopwords' : [\"proceeding\", \"proceedings\", \"proceed\", \"learn\", \"learning\", \"technique\", \"paper\", \"papers\", \"study\", \"conference\", \"analysis\", \"research\", \"ieee\", \"performance\", \"©\", \"all\", \"rights\", \"right\", \"reserve\", \"reserved\", \"propose\", \"declaration\", \"interest\"],\n",
        "    'lemmatize' : True\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFQYjQMQhf8_"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def preprocess_txt(text):\n",
        "    text = text.lower()\n",
        "    doc = nlp(text)\n",
        "    if config['remove_punct']:\n",
        "        doc = [token for token in doc if not token.is_punct]\n",
        "    if config['remove_num']:\n",
        "        doc = [token for token in doc if not token.is_digit]\n",
        "    if config['remove_stopwords']:\n",
        "        doc = [token for token in doc if not token.is_stop and token.text not in config['custom_stopwords']]\n",
        "    if config['lemmatize']:\n",
        "        doc = [token.lemma_ for token in doc]   # .lemma_ is a string\n",
        "\n",
        "    result = ''\n",
        "    for text in doc:\n",
        "        result += text + ' '\n",
        "\n",
        "    return result.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1K89ArYi1iC"
      },
      "outputs": [],
      "source": [
        "df['text'] = df['text'].apply(lambda text: preprocess_txt(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkqpL-_viyFs"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z1ZUQGMiq7_"
      },
      "source": [
        "# Save the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNaktbv3hsqb"
      },
      "outputs": [],
      "source": [
        "df.to_csv('ML_WIFI_preprocessed.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evCg6bjeXeGo"
      },
      "source": [
        "# Dataset and Metadata Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6XcrX8AUXGo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "FIGURE_FOLDER = 'figures'\n",
        "if not os.path.exists(FIGURE_FOLDER):\n",
        "    os.mkdir(FIGURE_FOLDER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beBYyAy4DpJs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('ML_WIFI_preprocessed.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjirerwK4wc0"
      },
      "outputs": [],
      "source": [
        "# brutal\n",
        "idxs = df['Year'].isna()\n",
        "for i, el in enumerate(idxs):\n",
        "    if el:\n",
        "        df['Year'].iloc[i] = df['Year'].iloc[i-1]\n",
        "df['Year'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePcq_jE7Vof-"
      },
      "outputs": [],
      "source": [
        "df['Year'] = df['Year'].astype(int)\n",
        "df['Cited by'] = df['Cited by'].astype(int)\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MePSV8UW38hc"
      },
      "outputs": [],
      "source": [
        "# paper count vs years\n",
        "count_all_df = df['Year'].value_counts().sort_index(ascending=True)\n",
        "count_all_df.index, count_all_df.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2IwV9lIsC1H"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-white')\n",
        "plt.rcParams['image.cmap'] = 'tab10'\n",
        "import numpy as np\n",
        "\n",
        "def smooth_list(l):\n",
        "    return [l[0]] + [np.mean(l[i-1: i+2]) for i, _ in enumerate(l[1:-1], 1)] + [l[-1]]\n",
        "\n",
        "\n",
        "# remove 2022 and 1992\n",
        "years, counts = count_all_df.index[1:-1], count_all_df.values[1:-1]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "plt.scatter(years, counts, linewidth=0.5, alpha=0.5)\n",
        "\n",
        "smoothed_counts = smooth_list(counts)\n",
        "plt.plot(years, smoothed_counts, linewidth=3)\n",
        "\n",
        "plt.xticks(ticks=[y for i, y in enumerate(years) if i%2 == 0], rotation=45, fontsize=21)\n",
        "plt.yticks([100 * i for i in range(8)], fontsize=21)\n",
        "plt.ylabel('Number of articles', fontsize=25)\n",
        "plt.savefig(FIGURE_FOLDER + '/papers_over_time.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJY4Ha-W_rp2"
      },
      "outputs": [],
      "source": [
        "# Paper types\n",
        "df['Document Type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp-NSHDMAPIe"
      },
      "outputs": [],
      "source": [
        "# most cited papers\n",
        "df.sort_values(by='Cited by', ascending=False).head(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlBayNb-JbZI"
      },
      "outputs": [],
      "source": [
        "df['Cited by'].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaFb4489sDUA"
      },
      "source": [
        "# Topic Modeling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bertopic"
      ],
      "metadata": {
        "id": "ymw2kOe6m0fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN"
      ],
      "metadata": {
        "id": "3yxc5EuRpxFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4LIGSo7DzgR"
      },
      "outputs": [],
      "source": [
        "umap_model = UMAP(n_neighbors=15, n_components=5,\n",
        "                  min_dist=0.0, metric='cosine', random_state=13)\n",
        "\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=60, metric='euclidean',\n",
        "                        cluster_selection_method='eom', prediction_data=True,\n",
        "                        min_samples=10)\n",
        "\n",
        "topic_model = BERTopic(verbose=True, embedding_model=\"all-MiniLM-L6-v2\",\n",
        "                       umap_model=umap_model, hdbscan_model=hdbscan_model,\n",
        "                       n_gram_range=(1, 3), calculate_probabilities=False)\n",
        "\n",
        "topics, _ = topic_model.fit_transform(df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GQFsPyqH_m6"
      },
      "outputs": [],
      "source": [
        "count_df = topic_model.get_topic_info()\n",
        "count_df['Perc'] = (count_df['Count'] / count_df['Count'].sum()).round(2)\n",
        "# rearrange columns\n",
        "new_order = [0, 2, 1, 3]\n",
        "data, cols = count_df.values[:, new_order], count_df.columns[new_order]\n",
        "count_df = pd.DataFrame(data=data, columns=cols)\n",
        "count_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYdv9_YWF9ca"
      },
      "outputs": [],
      "source": [
        "def print_topics(tm, topics):\n",
        "    for topic in set(topics):\n",
        "        print(f'topic : {topic}')\n",
        "        for el in tm.get_topic(topic):\n",
        "            print(el)\n",
        "\n",
        "print_topics(topic_model, topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejwTkXnHWTHg"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_barchart(topics=set(topics))\n",
        "plt.savefig(FIGURE_FOLDER + '/terms_per_topic.png', bbox_inches='tight', dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLHcyPZSUGMD"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgQFBDuWM3ey"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_hierarchy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "d3eT1ugisnCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxkFjOQIW60_"
      },
      "outputs": [],
      "source": [
        "topics_over_time = topic_model.topics_over_time(df['text'], df['Year'])\n",
        "\n",
        "topic_model.visualize_topics_over_time(topics_over_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65KoV8AlRCJ_"
      },
      "outputs": [],
      "source": [
        "topics_over_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf_DYpHbRPzh"
      },
      "outputs": [],
      "source": [
        "# topics over time is a dataframe with one row for each topic for each year\n",
        "topics_over_time = topics_over_time[(topics_over_time['Timestamp'] > 1965) & (topics_over_time['Timestamp'] < 2023)]\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "for t, t_name in zip(np.unique(topics_over_time['Topic']), np.unique(topics_over_time['Name'])):\n",
        "    t_df = topics_over_time[topics_over_time['Topic']==t]\n",
        "\n",
        "    plt.plot()\n",
        "    plt.plot(t_df['Timestamp'], t_df['Frequency'], linewidth=2.5, alpha=0.9, label=t_name)\n",
        "\n",
        "plt.yticks(fontsize=24)\n",
        "plt.xticks(fontsize=24)\n",
        "plt.ylabel('Frequency', fontsize=25)\n",
        "plt.legend(fontsize=15)\n",
        "plt.savefig(FIGURE_FOLDER + '/topic_per_time.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI9qRQ6BWpQv"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_heatmap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8OFRdr1uAs0"
      },
      "outputs": [],
      "source": [
        "topic_model.update_topics(df['text'], topics, n_gram_range=(2, 3))\n",
        "\n",
        "print_topics(topic_model, topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVbM4JK5USoM"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_hierarchy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWjG8xlQWW0y"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_barchart(topics=set(topics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi1-HvTxXMYR"
      },
      "source": [
        "# Find most representative docs per topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyTb_-XYXBOp"
      },
      "outputs": [],
      "source": [
        "abs_per_topic = topic_model.get_representative_docs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kplv-I6cXvBM"
      },
      "outputs": [],
      "source": [
        "index_per_topic = {}\n",
        "for key, val in abs_per_topic.items():\n",
        "    if key not in index_per_topic:\n",
        "        index_per_topic[key] = []\n",
        "    for abs in val:\n",
        "        # we assume just one match, hence we take the first element of the index\n",
        "        index_per_topic[key].append(df[df['text'] == abs].index[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMHhckzzYPIx"
      },
      "outputs": [],
      "source": [
        "index_per_topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqgT-y8HZbxL"
      },
      "outputs": [],
      "source": [
        "title_per_topic = {}\n",
        "for key, val in index_per_topic.items():\n",
        "    if key not in title_per_topic:\n",
        "        title_per_topic[key] = []\n",
        "    for idx in val:\n",
        "        title_per_topic[key].append(df['Title'].iloc[idx])\n",
        "title_per_topic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC5ZTNieaSxV"
      },
      "source": [
        "# Topic 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0s94bMKHoVC"
      },
      "outputs": [],
      "source": [
        "topic_n = 1\n",
        "topic_idx = [el == topic_n for el in topics]\n",
        "spec_topic = df.iloc[topic_idx]\n",
        "spec_topic.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diZ3xkQ_hLjD"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "def plot_wordcloud(text):\n",
        "    word_cloud = WordCloud(collocations = False, background_color = 'white').generate(text)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(word_cloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "text = ''.join(spec_topic['text'])\n",
        "plot_wordcloud(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKcC8fSjjO4Y"
      },
      "outputs": [],
      "source": [
        "# get most cited papers\n",
        "spec_topic.sort_values(by='Cited by', ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTJGK7pPGZOD"
      },
      "outputs": [],
      "source": [
        "# most representative docs\n",
        "spec_topic.loc[index_per_topic[topic_n]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPWTRAMGoNV"
      },
      "source": [
        "# Topic 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lG7eVi9Gqu8"
      },
      "outputs": [],
      "source": [
        "topic_n = 3\n",
        "topic_idx = [el == topic_n for el in topics]\n",
        "spec_topic = df.iloc[topic_idx]\n",
        "spec_topic.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjXT4eOHGxej"
      },
      "outputs": [],
      "source": [
        "text = ''.join(spec_topic['text'])\n",
        "plot_wordcloud(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga7iD1VYGzb-"
      },
      "outputs": [],
      "source": [
        "# get most cited papers\n",
        "spec_topic.sort_values(by='Cited by', ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "id_QswhBG1Bp"
      },
      "outputs": [],
      "source": [
        "# most representative docs\n",
        "spec_topic.loc[index_per_topic[topic_n]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hklz8M8yLwP4"
      },
      "source": [
        "# Topic 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N87yMtUS9eAG"
      },
      "outputs": [],
      "source": [
        "topic_n = 6\n",
        "topic_idx = [el == topic_n for el in topics]\n",
        "spec_topic = df.iloc[topic_idx]\n",
        "spec_topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZCf5zbs-LRv"
      },
      "outputs": [],
      "source": [
        "spec_topic.sort_values(by='Cited by', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJlYJ8Zm-SMO"
      },
      "outputs": [],
      "source": [
        "spec_topic['Year'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKAsIjwCaesG"
      },
      "source": [
        "# Topics Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsRb7aMdaeFZ"
      },
      "outputs": [],
      "source": [
        "topic_names = {\n",
        "    0: 'Indoor Localization',\n",
        "    1: 'ML for Improving Wireless Networks Performance',\n",
        "    2: 'IoT and Smart Houses',\n",
        "    3: 'Privacy and Intrusion detection',\n",
        "    4: 'Human Activity Recognition',\n",
        "    5: 'Human Condition Monitoring',\n",
        "    6: 'Wi-Fi and ML for robotics',\n",
        "    7: 'Gesture Recognition',\n",
        "    8: 'Crowd Counting and Monitoring',}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cVg3-uwCQKr"
      },
      "source": [
        "# Search most used ML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "517tnE-r-z6d"
      },
      "outputs": [],
      "source": [
        "possible_models_kwords = [['water'], ['waste']]\n",
        "models = {kwords_list[0] : [] for kwords_list in possible_models_kwords}\n",
        "\n",
        "for i, abs in enumerate(df['text']):\n",
        "    for kwords_list in possible_models_kwords:\n",
        "        for kword in kwords_list:\n",
        "            if kword in abs:\n",
        "                models[kwords_list[0]].append(i)\n",
        "                break\n",
        "\n",
        "{model : len(models[model]) for model in models}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoIcRemmfXsg"
      },
      "outputs": [],
      "source": [
        "len(models.pop('reinforcement learning'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im_hxmvGGlYD"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_per_year = {model : sorted([df['Year'].iloc[i] for i in idxs]) for model, idxs in models.items()}\n",
        "\n",
        "legend = []\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "for i, (model, years) in enumerate(model_per_year.items()):\n",
        "    legend.append(model)\n",
        "    count_years = pd.Series(years).value_counts().sort_index()\n",
        "    yrs, vls = count_years.index, count_years.values\n",
        "    # delete oldest articles\n",
        "    new_yrs, new_vls = [], []\n",
        "    for y, v in zip(yrs, vls):\n",
        "        if y > 1975 and y != 2023:\n",
        "            new_yrs.append(y)\n",
        "            new_vls.append(v)\n",
        "    plt.plot(new_yrs, new_vls, linewidth=2.5, alpha=0.9)\n",
        "\n",
        "plt.yticks(fontsize=24)\n",
        "plt.xticks(fontsize=24)\n",
        "plt.ylabel('Number of occurences', fontsize=25)\n",
        "plt.legend(legend, fontsize=22)\n",
        "plt.savefig(FIGURE_FOLDER + '/models_per_years_nn.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmI9elElJ6K8"
      },
      "outputs": [],
      "source": [
        "# same without neural\n",
        "model_per_year = {model : sorted([df['Year'].iloc[i] for i in idxs]) for model, idxs in models.items()}\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "for i, (model, years) in enumerate(model_per_year.items()):\n",
        "    if model != 'neural':\n",
        "        count_years = pd.Series(years).value_counts().sort_index()\n",
        "        yrs, vls = count_years.index, count_years.values\n",
        "        # delete oldest articles\n",
        "        new_yrs, new_vls = [], []\n",
        "        for y, v in zip(yrs, vls):\n",
        "            if 2005 < y < 2022:\n",
        "                new_yrs.append(y)\n",
        "                new_vls.append(v)\n",
        "        plt.plot(new_yrs, new_vls, label=model, linewidth=2.5, alpha=0.9)\n",
        "\n",
        "plt.yticks(fontsize=24)\n",
        "plt.xticks(fontsize=24)\n",
        "plt.ylabel('Number of occurences', fontsize=25)\n",
        "plt.legend(legend, fontsize=22)\n",
        "plt.savefig(FIGURE_FOLDER + '/models_per_years.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbpB6ao3XAlo"
      },
      "outputs": [],
      "source": [
        "model_list_per_topic = {topic : [] for topic in set(topics)}\n",
        "\n",
        "for model, idxs in models.items():\n",
        "    for i in idxs:\n",
        "        model_list_per_topic[topics[i]].append(model)\n",
        "model_list_per_topic[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRe1q6ihFcHn"
      },
      "outputs": [],
      "source": [
        "def count_val(list_el):\n",
        "    res = {}\n",
        "    for el in list_el:\n",
        "        if el not in res:\n",
        "            res[el] = 0\n",
        "        res[el] += 1\n",
        "    return res\n",
        "\n",
        "model_count_per_topic = {key : count_val(model_list) for key, model_list in model_list_per_topic.items()}\n",
        "model_count_per_topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pH0XVOwLITQ"
      },
      "outputs": [],
      "source": [
        "# memento count_df[['Topic', 'Count']] contains topic sizes\n",
        "# merge topic 0 and 7\n",
        "def merge_counts(c1, c2):\n",
        "    for k, v in c2.items():\n",
        "        if k in c1.keys():\n",
        "            c1[k] += c2[k]\n",
        "        else:\n",
        "            c1[k] = c2[k]\n",
        "    return c1\n",
        "\n",
        "model_count_per_topic[0] = merge_counts(model_count_per_topic[0], model_count_per_topic[7])\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "cmap = 'Greens'\n",
        "\n",
        "model_names = list(models.keys())\n",
        "perc_matr = np.empty((len(set(topics)) - 2, len(model_names)))  # remove topic 7\n",
        "\n",
        "for topic, val_dict in model_count_per_topic.items():\n",
        "    if topic != -1 and topic != 7:\n",
        "        topic_size = count_df[count_df['Topic'] == topic]['Count'].values[0]\n",
        "        for model in model_names:\n",
        "            model_idx = list(models.keys()).index(model)\n",
        "            tmp_index = topic if topic < 7 else topic - 1\n",
        "            perc_matr[tmp_index, model_idx] = val_dict.get(model, 0) / topic_size * 100\n",
        "\n",
        "yticks = [topic_names[t] for t in set(topics) if t!=-1 and t != 7]\n",
        "\n",
        "sns.heatmap(perc_matr, xticklabels=model_names, yticklabels=yticks, cmap=cmap)\n",
        "plt.savefig(FIGURE_FOLDER + '/ml_models_heatmap_nn.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnXOxrF2b4Zo"
      },
      "outputs": [],
      "source": [
        "idxs = [i for i, el in enumerate(model_names) if el != 'neural']\n",
        "perc_matr_no_NN = perc_matr[:, idxs]\n",
        "model_names_no_NN = [el for el in model_names if el != 'neural']\n",
        "\n",
        "sns.heatmap(perc_matr_no_NN, xticklabels=model_names_no_NN, yticklabels=yticks, cmap=cmap)\n",
        "plt.savefig(FIGURE_FOLDER + '/ml_models_heatmap.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo3HCGRLq0J_"
      },
      "source": [
        "# Search specific terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCqQHt9YC_8I"
      },
      "outputs": [],
      "source": [
        "topic_names"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "noVYvJ2ulA7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-CKnt_szq0L"
      },
      "outputs": [],
      "source": [
        "topic_model.find_topics(\"csi\", top_n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuXdo6L7q95u"
      },
      "outputs": [],
      "source": [
        "topic_model.find_topics(\"rss\", top_n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxeEJdDCq-sr"
      },
      "outputs": [],
      "source": [
        "topic_model.find_topics(\"rssi\", top_n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGVKtk53q_Z7"
      },
      "outputs": [],
      "source": [
        "topic_model.find_topics(\"probe\", top_n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4EehmmtC6UV"
      },
      "outputs": [],
      "source": [
        "topic_model.find_topics(\"probe request\", top_n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGMNn9_tC8ek"
      },
      "outputs": [],
      "source": [
        "def count_word(df, substr):\n",
        "    if type(substr) == str:\n",
        "        substr = [substr]\n",
        "\n",
        "    cnt_per_year = {}\n",
        "    for year, el in zip(df['Year'], df['text']):\n",
        "        if year not in cnt_per_year.keys():\n",
        "            cnt_per_year[year] = 0\n",
        "        #cnt_per_year[year] += el.lower().count(substr)\n",
        "        for sub in substr:\n",
        "            if el.lower().count(sub) > 0:\n",
        "                cnt_per_year[year] += 1\n",
        "                break\n",
        "\n",
        "    return cnt_per_year\n",
        "\n",
        "csi = count_word(df, 'csi')\n",
        "csi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc8Dtu5nEWib"
      },
      "outputs": [],
      "source": [
        "rss = count_word(df, 'rss')\n",
        "rss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_0WjgnHGGoQ"
      },
      "outputs": [],
      "source": [
        "def plot_word_count(names, use_log, *args, filename=None):\n",
        "    MARKERS = ['o', 'x', 's', 'd']\n",
        "    years = sorted(list(args[0].keys()))[1:-1]    # remove 1992 and 2022\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    alpha = 0.7\n",
        "    lw = 3\n",
        "\n",
        "    for i, arg in enumerate(args):\n",
        "        arg_count = [np.log(arg[y]) if use_log and arg[y] != 0 else arg[y] for y in years]\n",
        "        arg_smooth = smooth_list(arg_count)\n",
        "\n",
        "        plt.scatter(years, arg_count, alpha=alpha, marker=MARKERS[i])\n",
        "        plt.plot(years, arg_smooth, linewidth=lw)\n",
        "\n",
        "    plt.xticks(ticks=[y for i, y in enumerate(years) if i%2 == 0], rotation=45, fontsize=21)\n",
        "    plt.yticks(fontsize=21)\n",
        "    if use_log:\n",
        "        plt.ylabel('Log of the number of articles', fontsize=25)\n",
        "    else:\n",
        "        plt.ylabel('Number of articles', fontsize=25)\n",
        "    plt.legend(names, prop={'size': 25})\n",
        "    if filename:\n",
        "        plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnWn7gOtTqIS"
      },
      "outputs": [],
      "source": [
        "plt.savefig(FIGURE_FOLDER + '/csi_rss.png', bbox_inches='tight', dpi=300)\n",
        "\n",
        "plot_word_count(['RSSI', 'CSI'], False, rss, csi, filename=FIGURE_FOLDER + '/prova.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nf8em6KKBrJ"
      },
      "outputs": [],
      "source": [
        "plot_word_count(['RSSI', 'CSI'], True, rss, csi, filename=FIGURE_FOLDER + '/log_csi_rss.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j0hdH-nH2MQ"
      },
      "outputs": [],
      "source": [
        "passive = count_word(df, ['probe', 'mac addr', 'randomized'])#, 'sensing'])\n",
        "passive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z2kRq8f_TgC"
      },
      "outputs": [],
      "source": [
        "sum(passive.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t96xPsUbItCl"
      },
      "outputs": [],
      "source": [
        "plot_word_count(['passive'], False, passive, filename=FIGURE_FOLDER + '/passive_count.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15LjLMv8Yh5_"
      },
      "outputs": [],
      "source": [
        "idxs = []\n",
        "for i, text in enumerate(df['text']):\n",
        "    if text.count('probe') > 0 or text.count('randomiz') > 0:\n",
        "        idxs.append(i)\n",
        "\n",
        "df.iloc[idxs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNBoC3zI43s9"
      },
      "outputs": [],
      "source": [
        "len(idxs), df.iloc[idxs]['Cited by'].sum(), df.iloc[idxs]['Cited by'].sum() / len(idxs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkzX0GrM5Czh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}